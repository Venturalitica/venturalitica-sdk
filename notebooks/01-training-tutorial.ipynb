{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Venturalitica Tutorial: Training with Governance\n",
                "\n",
                "This notebook demonstrates how to integrate fairness and performance checks into your ML workflow using **Venturalitica SDK**.\n",
                "\n",
                "### Objectives:\n",
                "1. **Pre-training Audit**: Detect data bias before training.\n",
                "2. **Training**: Train a basic model while tracking duration and emissions.\n",
                "3. **Post-training Audit**: Verify model fairness and performance on test data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from ucimlrepo import fetch_ucirepo\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "import venturalitica as vl\n",
                "import pandas as pd\n",
                "import yaml\n",
                "\n",
                "print(\"Venturalitica version:\", vl.__version__ if hasattr(vl, '__version__') else '0.2.0') "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data\n",
                "We use the **UCI German Credit** dataset, a classic benchmark for credit scoring fairness."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = fetch_ucirepo(id=144)\n",
                "df = dataset.data.features.copy()\n",
                "df['class'] = dataset.data.targets\n",
                "\n",
                "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
                "print(f\"Loaded {len(df)} samples.\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Defining Policies (OSCAL)\n",
                "We'll define a simple policy to check for **Class Imbalance** and **Disparate Impact** across Gender and Age."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "policy_yaml = \"\"\"\n",
                "assessment-plan:\n",
                "  metadata:\n",
                "    title: \"Credit Governance Policy\"\n",
                "  reviewed-controls:\n",
                "    control-implementations:\n",
                "      - description: \"Bias and Performance rules\"\n",
                "        implemented-requirements:\n",
                "          - control-id: class-balance\n",
                "            description: \"Minority class representation\"\n",
                "            props:\n",
                "              - name: metric_key\n",
                "                value: class_imbalance\n",
                "              - name: threshold\n",
                "                value: \"0.2\"\n",
                "              - name: operator\n",
                "                value: gt\n",
                "          - control-id: gender-disparate\n",
                "            description: \"Gender fairness (DI > 0.8)\"\n",
                "            props:\n",
                "              - name: metric_key\n",
                "                value: disparate_impact\n",
                "              - name: threshold\n",
                "                value: \"0.8\"\n",
                "              - name: operator\n",
                "                value: gt\n",
                "              - name: \"input:dimension\"\n",
                "                value: gender\n",
                "          - control-id: age-disparate\n",
                "            description: \"Age fairness (DI > 0.5)\"\n",
                "            props:\n",
                "              - name: metric_key\n",
                "                value: disparate_impact\n",
                "              - name: threshold\n",
                "                value: \"0.5\"\n",
                "              - name: operator\n",
                "                value: gt\n",
                "              - name: \"input:dimension\"\n",
                "                value: age\n",
                "          - control-id: accuracy-score\n",
                "            description: \"Model utility (Accuracy > 70%)\"\n",
                "            props:\n",
                "              - name: metric_key\n",
                "                value: accuracy_score\n",
                "              - name: threshold\n",
                "                value: \"0.7\"\n",
                "              - name: operator\n",
                "                value: gt\n",
                "\"\"\"\n",
                "\n",
                "with open(\"policy.oscal.yaml\", \"w\") as f:\n",
                "    f.write(policy_yaml)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Pre-Training Audit\n",
                "We run the audit on the **training set** to ensure our data foundation is fair."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "vl.enforce(\n",
                "    data=train_df,\n",
                "    target=\"class\",\n",
                "    gender=\"Attribute9\",\n",
                "    age=\"Attribute13\",\n",
                "    policy=\"policy.oscal.yaml\"\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Train with Monitoring\n",
                "We'll use `vl.monitor` to track the training metadata and emissions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pre-processing\n",
                "df_encoded = pd.get_dummies(df.drop(columns=['class']))\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    df_encoded, \n",
                "    df['class'].values.ravel(), \n",
                "    test_size=0.2, \n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
                "\n",
                "with vl.monitor(name=\"RandomForest-Credit\"):\n",
                "    model.fit(X_train, y_train)\n",
                "    \n",
                "predictions = model.predict(X_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Post-Training Audit\n",
                "Finally, we verify if the trained model satisfies our fairness and performance requirements on the test set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "audit_df = df.iloc[X_test.index].copy()\n",
                "audit_df['prediction'] = predictions\n",
                "\n",
                "vl.enforce(\n",
                "    data=audit_df,\n",
                "    target=\"class\",\n",
                "    prediction=\"prediction\",\n",
                "    gender=\"Attribute9\",\n",
                "    age=\"Attribute13\",\n",
                "    policy=\"policy.oscal.yaml\"\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "- **Data Audit**: Detected imbalance and age bias.\n",
                "- **Model Training**: Successfully monitored.\n",
                "- **Model Audit**: Verified that fairness might have degraded post-training (Bias Amplification)."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}