# Inicio R√°pido en 60 Segundos

**Objetivo**: Tu primera auditor√≠a de sesgo en menos de 60 segundos.

---
## Los fundamentos: Del Riesgo al C√≥digo

Construir una IA de Alto Riesgo requiere un cambio fundamental en c√≥mo abordamos las pruebas. Ya no es suficiente verificar la precisi√≥n t√©cnica (por ejemplo, F1 Score); ahora debemos probar matem√°ticamente que el sistema respeta los derechos fundamentales, como la no discriminaci√≥n o la calidad de los datos, tal como lo exige la **Ley de IA de la UE**.

Ventural√≠tica automatiza esto tratando la "Gobernanza" como una dependencia. En lugar de vagos requisitos legales, defines pol√≠ticas estrictas (OSCAL) que tu modelo debe aprobar antes de ser desplegado. Esto convierte el cumplimiento en un problema de ingenier√≠a determinista.

!!! question "¬øEs mi Sistema de Alto Riesgo?"
    Seg√∫n el [**Art√≠culo 6**](https://artificialintelligenceact.eu/es/article/6/) de la Ley de IA de la UE, un sistema es de Alto Riesgo si est√° cubierto por el [**Anexo I**](https://artificialintelligenceact.eu/es/annex/1/) (Componentes de Seguridad como maquinaria/dispositivos m√©dicos) o listado en el [**Anexo III**](https://artificialintelligenceact.eu/es/annex/3/) (Biometr√≠a, Infraestructura Cr√≠tica, Educaci√≥n, Empleo, Servicios Esenciales, Cumplimiento de la Ley, Migraci√≥n, Justicia/Democracia).

**La Capa de Traducci√≥n:**

1.  **Riesgo Fundamental**: "El modelo no debe discriminar a grupos protegidos" (Art 9).

2.  **Control de Pol√≠tica**: "La Tasa de Impacto Dispar debe ser > 0.8".

3.  **Aserci√≥n de C√≥digo**: `assert calculated_metric > 0.8`.

Cuando ejecutas `quickstart()`, t√©cnicamente est√°s ejecutando una **Prueba Unitaria de √âtica**.

---
## Paso 1: Instalaci√≥n

```bash
pip install git+https://github.com/Venturalitica/venturalitica-sdk.git
```

---

## Paso 2: Ejecuta Tu Primera Auditor√≠a

```python
import venturalitica as vl

vl.quickstart('loan')
```

**Salida:**

```text
[Ventural√≠tica {{ version }}] üéì Escenario: Equidad en Calificaci√≥n Crediticia
[Ventural√≠tica {{ version }}] üìä Cargado: UCI Dataset #144 (1000 muestras)

  CONTROL                DESCRIPCION                            ACTUAL     LIMITE     RESULTADO
  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  credit-data-imbalance  Calidad de Datos                       0.431      > 0.2      ‚úÖ PASS
  credit-data-bias       Impacto Dispar                         0.836      > 0.8      ‚úÖ PASS
  credit-age-disparate   Disparidad por Edad                    0.361      > 0.5      ‚ùå FAIL
  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  Resumen de Auditor√≠a: ‚ùå VIOLACI√ìN | 2/3 controles pasados
```

!!! info
    La auditor√≠a detect√≥ un sesgo basado en la edad en el conjunto de datos de Cr√©dito Alem√°n UCI.

## Paso 3: Qu√© Sucede Bajo el Cap√≥

La funci√≥n `quickstart()` es un envoltorio que realiza el ciclo de vida completo de cumplimiento de una sola vez:

1.  **Descarga Datos**: Obtiene el conjunto de datos de Cr√©dito Alem√°n UCI.
2.  **Carga Pol√≠tica**: Lee `risks.oscal.yaml` que define las reglas de equidad.
3.  **Ejecuta**: Corre la auditor√≠a (`vl.enforce`).
4.  **Registra**: Captura la evidencia (`trace.json`) para el panel de control.

Aqu√≠ est√° el c√≥digo "manual" equivalente:

```python
from ucimlrepo import fetch_ucirepo
import venturalitica as vl

# 1. Cargar Datos (La "Fuente de Riesgo")
dataset = fetch_ucirepo(id=144)
df = dataset.data.features
df['class'] = dataset.data.targets

# 2. Definir la Pol√≠tica (La "Ley")
# Cargamos una pre-definida policies/risks.oscal.yaml

# 3. Ejecutar la Auditor√≠a (La "Prueba")
# Esto genera autom√°ticamente la Lista de Materiales de Evidencia (BOM)
with vl.tracecollector("manual_audit"):
    vl.enforce(
        data=df,
        target="class",          # El resultado (True/False)
        gender="Attribute9",     # Grupo Protegido A
        age="Attribute13",       # Grupo Protegido B
        policy="risks.oscal.yaml"
    )
```

### La L√≥gica de la Pol√≠tica

La pol√≠tica (`risks.oscal.yaml`) es el puente. Le dice al SDK *qu√©* verificar para que no tengas que codificarlo.

```yaml
# ... dentro del YAML OSCAL ...
- control-id: credit-data-bias
  description: "La tasa de impacto dispar debe ser > 0.8 (regla del 80%)"
  props:
    - name: metric_key
      value: disparate_impact   # <--- La Funci√≥n Python a llamar
    - name: threshold
      value: "0.8"              # <--- El L√≠mite a aplicar
    - name: operator
      value: ">"                # <--- La L√≥gica (> 0.8)
    - name: "input:dimension"
      value: gender             # <--- Mapea a "Attribute9"
```

Este dise√±o desacopla la **Gobernanza** (el archivo de pol√≠tica) de la **Ingenier√≠a** (el c√≥digo python).

---

## Por Qu√© Importa Esto

Sin este mecanismo, tu modelo de IA es una "Caja Negra" legal:

*   **Responsabilidad Civil**: No puedes probar que verificaste el sesgo *antes* del despliegue (Art 9).
*   **Fragilidad**: El cumplimiento es una lista de verificaci√≥n manual, f√°cil de olvidar u omitir.
*   **Opacidad**: Los auditores no pueden ver el v√≠nculo entre tu c√≥digo y la ley.

Al ejecutar `quickstart()`, acabas de generar un **Artefacto de Cumplimiento** inmutable. Incluso si las leyes cambian, tu evidencia permanece.

## Paso 4: El Panel de Control "Caja de Cristal" üìä

Ahora que tenemos la evidencia (la grabaci√≥n de la "Caja Negra"), inspeccion√©mosla en el **Mapa Regulatorio**.

```bash
venturalitica ui
```

Navega a trav√©s de las pesta√±as del **Mapa de Cumplimiento**:

*   **Art√≠culo 9 (Riesgo)**: Ve el control fallido `credit-age-disparate`. Esta es tu evidencia t√©cnica de "Monitoreo de Riesgos".
*   **Art√≠culo 10 (Datos)**: Ve la distribuci√≥n de datos y verificaciones de calidad.
*   **Art√≠culo 13 (Transparencia)**: Revisa el "Feed de Transparencia" para ver tus dependencias de Python (BOM).

---

## Paso 5: Generar Documentaci√≥n (Anexo IV) üìù

El paso final es convertir esta evidencia en un documento legal.

1.  En el Panel, ve a la pesta√±a **"Generaci√≥n"**.
2.  Selecciona **"Espa√±ol"**.
3.  Haz clic en **"Generar Anexo IV"**.

Ventural√≠tica redactar√° un documento t√©cnico que hace referencia a tu ejecuci√≥n espec√≠fica:

> *"Como se evidencia en `trace_quickstart_loan.json`, el sistema fue auditado contra **[Pol√≠tica OSCAL: Equidad en Calificaci√≥n Crediticia]**. Se detect√≥ una desviaci√≥n en la Disparidad de Edad (0.36), identificando un riesgo potencial de sesgo..."*

### Referencias
*   **Pol√≠tica Usada**: [`loan/risks.oscal.yaml`](https://github.com/venturalitica/venturalitica-sdk-samples/blob/main/policies/loan/risks.oscal.yaml)
*   **Base Legal**:
    *   [Ley de IA de la UE Art√≠culo 9 (Gesti√≥n de Riesgos)](https://artificialintelligenceact.eu/es/article/9/)
    *   [Ley de IA de la UE Art√≠culo 11 (Documentaci√≥n T√©cnica)](https://artificialintelligenceact.eu/es/article/11/)

## ¬øQu√© sigue?

- **[Referencia de API](api.md)** - Documentaci√≥n completa
- **Crea tu propia pol√≠tica** - Copia el YAML anterior y modifica los umbrales
