<SYSTEM>This is the abridged developer documentation for Venturalítica</SYSTEM>

# Venturalítica AI Assurance SDK

> Compliance-as-Code for High-Risk AI. Transform Python code into Legal Evidence.

**AI Assurance through Compliance-as-Code.** Venturalítica transforms your Python code into **Legal Evidence**. It automatically maps your technical metrics, data audits, and execution logs to the **EU AI Act (Articles 9-15)** without leaving your local environment. *** ## Quickstart in 60 Seconds [Section titled “Quickstart in 60 Seconds”](#quickstart-in-60-seconds) * Core ```bash pip install venturalitica ``` * Fairness ```bash pip install "venturalitica[metrics]" ``` * Dashboard ```bash pip install "venturalitica[dashboard]" ``` * Agentic ```bash pip install "venturalitica[agentic]" ``` * Full ```bash pip install "venturalitica[full]" ``` Detect bias in your datasets or models with one line of code: ```python import venturalitica as vl # Run a full AI Assurance audit on the built-in loan scenario results = vl.quickstart('loan') ``` Then launch the **AI Assurance Dashboard** to explore results: ```bash venturalitica ui ``` *** ## Key Features [Section titled “Key Features”](#key-features) | Feature | Description | | :---------------------- | :----------------------------------------------------------------------------- | | **`enforce()`** | Audit datasets and models against OSCAL policies with 35+ built-in metrics. | | **`monitor()`** | Wrap training runs with automatic evidence collection (BOM, hardware, carbon). | | **Glass Box Dashboard** | 4-phase regulatory workflow: Identity, Policy, Verify, Report. | | **Policy as Code** | Define assurance rules in OSCAL `assessment-plan` format. | | **Column Binding** | Decouple policies from schemas via synonym-based column resolution. | | **Local Sovereignty** | Zero-cloud dependency. All enforcement runs locally. | | **Annex IV** | Auto-draft technical documentation from local traces. | *** ## Architecture [Section titled “Architecture”](#architecture) ```text ┌─────────────────────────────────────────────────────────────┐ │ Your ML Pipeline │ │ │ │ DataFrame ──► vl.enforce(policy=…) ──► ComplianceResult[] │ │ │ │ │ OSCAL Policy │ │ (assessment-plan) │ └───────────┬─────────────────────────────────┬───────────────┘ │ │ ┌──────▼──────┐ ┌──────▼──────┐ │ 35+ Metrics │ │ 7 Probes │ │ ─────────── │ │ ───────── │ │ Fairness │ │ Hardware │ │ Data Quality│ │ Carbon │ │ Performance │ │ BOM │ │ Privacy │ │ Artifact │ │ Robustness │ │ Trace │ └──────┬──────┘ └──────┬──────┘ │ │ └───────────────┬─────────────────┘ │ ┌──────▼──────┐ │ Dashboard │ │ ───────── │ │ Identity │ │ Policy │ │ Verify │ │ Report │ └─────────────┘ ``` *** ## Documentation [Section titled “Documentation”](#documentation) | Guide | Description | | :------------------------------------------------- | :------------------------------------------------------------------- | | **[Quickstart](/quickstart/)** | Run a full compliance scan in 2 minutes. | | **[API Reference](/reference/api/)** | `enforce()`, `monitor()`, `wrap()`, `quickstart()`, `PolicyManager`. | | **[Metrics Reference](/reference/metrics/)** | All 35+ metrics across 7 categories. | | **[Policy Authoring](/guides/policy-authoring/)** | Write OSCAL policies from scratch. | | **[Dashboard Guide](/guides/dashboard/)** | The Glass Box 4-phase workflow. | | **[Column Binding](/guides/column-binding/)** | Map abstract names to DataFrame columns. | | **[Probes Reference](/reference/probes/)** | 7 evidence probes for EU AI Act compliance. | | **[Experimental Features](/guides/experimental/)** | CLI `login`/`pull`/`push` (SaaS preview). | ### Academy (Step-by-Step Learning) [Section titled “Academy (Step-by-Step Learning)”](#academy-step-by-step-learning) | Level | Role | Focus | | :----------------------------------------- | :-------------- | :--------------------------------------------------- | | **[Level 1](/academy/level1-policy/)** | Policy Author | Write your first OSCAL policy for the loan scenario. | | **[Level 2](/academy/level2-integrator/)** | Integrator | Add `enforce()` to your ML pipeline. | | **[Level 3](/academy/level3-auditor/)** | Auditor | Review evidence and interpret results. | | **[Level 4](/academy/level4-annex-iv/)** | Compliance Lead | Generate Annex IV technical documentation. | ### Tutorials [Section titled “Tutorials”](#tutorials) * **[Writing Code-First Policy](/tutorials/writing-policy/)** — Translate legal requirements into OSCAL controls. *** ## Installation [Section titled “Installation”](#installation) * Core ```bash pip install venturalitica ``` * Fairness ```bash pip install "venturalitica[metrics]" ``` Adds [Fairlearn](https://fairlearn.org/) for advanced fairness metrics. * Dashboard ```bash pip install "venturalitica[dashboard]" ``` Adds [Streamlit](https://streamlit.io/) for the Glass Box dashboard. * Agentic ```bash pip install "venturalitica[agentic]" ``` Adds LangGraph, LangChain, and llama-cpp-python for agent workflows. * Green ```bash pip install "venturalitica[green]" ``` Adds [CodeCarbon](https://codecarbon.io/) for energy consumption tracking. * Full ```bash pip install "venturalitica[full]" ``` Installs all optional packages (metrics + dashboard + agentic + green + torch). Requires Python 3.11+. *** ## Links [Section titled “Links”](#links) [Quickstart Guide](/quickstart/) | [API Reference](/reference/api/) | [GitHub](https://github.com/venturalitica/venturalitica-sdk) * **Found a bug or want to propose a feature?** Open a **[GitHub Issue](https://github.com/venturalitica/venturalitica-sdk/issues/new)**. (c) 2026 Venturalitica | Built for Responsible AI

# Zero to Pro: The 5-Minute Journey

> Transform from Python Developer to AI Assurance Engineer in 3 steps.

**Goal**: Transform from “Python Developer” to “AI Assurance Engineer” in 3 steps. *** ## The Philosophy: Compliance as Code [Section titled “The Philosophy: Compliance as Code”](#the-philosophy-compliance-as-code) You are used to `pytest` for checking if your function adds 2+2 correctly. But how do you test if your AI model respects **Human Rights**? Venturalitica treats “Assurance” as a dependency. Instead of vague legal advice, you define stricter **Policies (OSCAL)**. Your CI/CD pipeline enforces them just like linter rules. ### The Curriculum [Section titled “The Curriculum”](#the-curriculum) | Level | Role | Goal | | :----------------------------------------- | :------------- | :------------------------------------------- | | **[Start Here](#step-1-install)** | **Developer** | Run your first audit in < 60s. | | **[Level 1](/academy/level1-policy/)** | **Engineer** | **Implement Controls** for identified Risks. | | **[Level 2](/academy/level2-integrator/)** | **Integrator** | **Viz & MLOps**: “Compliance as Metadata”. | | **[Level 3](/academy/level3-auditor/)** | **Auditor** | Proof: “Trust the Glass Box”. | | **[Level 4](/academy/level4-annex-iv/)** | **Architect** | GenAI Docs: “Annex IV”. | *** ## Step 1: Install [Section titled “Step 1: Install”](#step-1-install) We recommend **uv** for blazing speed, or standard `pip`. * uv ```bash uv pip install venturalitica ``` * pip ```bash pip install venturalitica ``` ## Step 2: Get the Code [Section titled “Step 2: Get the Code”](#step-2-get-the-code) To follow the **Academy**, clone the samples repository. This will be your working directory for all levels. ```bash git clone https://github.com/venturalitica/venturalitica-sdk-samples.git cd venturalitica-sdk-samples/scenarios/loan-credit-scoring ``` ## Step 3: Run Your First Audit [Section titled “Step 3: Run Your First Audit”](#step-3-run-your-first-audit) Run this single line of code. It downloads a dataset, loads a policy, and audits a model. ```python import venturalitica as vl # Run the 'loan' scenario vl.quickstart('loan') ``` **Output:** ```text CONTROL DESCRIPTION ACTUAL LIMIT RESULT ──────────────────────────────────────────────────────────────────────────────────────────────── credit-data-imbalance Data Quality 0.429 > 0.2 PASS credit-data-bias Disparate impact 0.818 > 0.8 PASS credit-age-disparate Age disparity 0.286 > 0.5 FAIL ──────────────────────────────────────────────────────────────────────────────────────────────── Audit Summary: VIOLATION | 2/3 controls passed ``` ### Take Home Message [Section titled “Take Home Message”](#take-home-message) ## Step 4: Choose Your Path [Section titled “Step 4: Choose Your Path”](#step-4-choose-your-path) Now that you’ve seen the failure, learn how to fix it and verify it. * **[Level 1: The Engineer](/academy/level1-policy/)** — Learn how to implement **Controls** that mitigate identified Risks. **Detect & Block** non-compliant models. * **[Level 2: The Integrator](/academy/level2-integrator/)** — Log outcomes to MLOps tools and verify results visually in the **Dashboard**. * **[Level 3: The Auditor](/academy/level3-auditor/)** — Learn how to perform a “Glass Box” audit on the loan model and generate cryptographic proofs. * **[Level 4: The Architect](/academy/level4-annex-iv/)** — The Boss Level. Train a high-risk financial model and generate the massive Technical Documentation required by the EU AI Act. *** ## External References [Section titled “External References”](#external-references) * **EU AI Act**: [Full Legal Text (EUR-Lex)](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52021PC0206) * **ISO 42001**: [Artificial Intelligence Management System (AIMS)](https://www.iso.org/standard/81230.html) * **NIST AI RMF**: [Risk Management Framework 1.0](https://www.nist.gov/itl/ai-risk-management-framework)

# Level 1: The Engineer (Policy & Configuration)

> Learn how to implement Controls that mitigate Risks using OSCAL policies.

**Goal**: Learn how to implement **Controls** that mitigate **Risks**. **Prerequisite**: [Zero to Pro (Academy Index)](/academy/) *** ## 1. The Scenario: From Risk to Control [Section titled “1. The Scenario: From Risk to Control”](#1-the-scenario-from-risk-to-control) In a formal Management System (**ISO 42001**), assurance follows a top-down flow: 1. **Risk Assessment**: The Compliance Officer (CO) identifies a business risk (e.g., *“Our lending AI might discriminate against elderly applicants, causing legal and reputational damage”*). 2. **Control Definition**: To mitigate this risk, the CO sets a **Control** (e.g., *“The Age Disparity Ratio must always be > 0.5”*). 3. **Technical Implementation**: That’s your job. You take the CO’s requirement and turn it into the technical “Law” (**Article 10: Data Assurance**). In the [Zero to Pro](/academy/) quickstart, `vl.quickstart('loan')` FAILED: ```text credit-age-disparate Age disparity 0.286 > 0.5 FAIL ``` ### What happened? [Section titled “What happened?”](#what-happened) The **Control** successfully detected a **Compliance Gap**. The “Reality” of the data (`0.286`) violated the requirement set to mitigate the “Age Bias” risk. If you lower the threshold to 0.3 just to make the test “pass,” you aren’t fixing the code — you are **bypassing a security control** and exposing the company to the original risk. ## 2. Anatomy of a Control (OSCAL) [Section titled “2. Anatomy of a Control (OSCAL)”](#2-anatomy-of-a-control-oscal) Your job is to translate the CO’s requirement into Code. Create a file named `data_policy.oscal.yaml` (or [download it from GitHub](https://github.com/venturalitica/venturalitica-sdk-samples/blob/main/scenarios/loan-credit-scoring/policies/loan/data_policy.oscal.yaml)). The canonical format is `assessment-plan`. Here is the full policy with **3 controls** — copy-paste this into your project: ```yaml assessment-plan: metadata: title: Credit Risk Assessment Policy (German Credit) version: "1.1" control-implementations: - description: Credit Scoring Fairness Controls implemented-requirements: # Control 1: Class Imbalance # "Rejected loans must be >= 20% of the dataset" - control-id: credit-data-imbalance description: > Data Quality: Minority class (rejected loans) should represent at least 20% of the dataset to avoid biased training. props: - name: metric_key value: class_imbalance - name: threshold value: "0.2" - name: operator value: gt - name: "input:target" value: target # Control 2: Gender Fairness (Four-Fifths Rule) # "Loan approvals must not favor one gender > 80%" - control-id: credit-data-bias description: > Pre-training Fairness: Disparate impact ratio should follow the standard '80% Rule' (Four-Fifths Rule). props: - name: metric_key value: disparate_impact - name: threshold value: "0.8" - name: operator value: gt - name: "input:target" value: target - name: "input:dimension" value: gender # Control 3: Age Fairness # "Loan approvals must not discriminate by age > 50%" - control-id: credit-age-disparate description: "Disparate impact ratio for raw age" props: - name: metric_key value: disparate_impact - name: threshold value: "0.50" - name: operator value: gt - name: "input:target" value: target - name: "input:dimension" value: age ``` ### What each prop does [Section titled “What each prop does”](#what-each-prop-does) | Property | Purpose | Example | | :----------------- | :---------------------------------------------------------------------- | :----------------------------------------------------------------------- | | `metric_key` | Which metric to compute (from [Metrics Reference](/reference/metrics/)) | `disparate_impact`, `class_imbalance`, `accuracy_score` | | `threshold` | The numeric boundary | `"0.8"` | | `operator` | Comparison operator: `gt`, `gte`, `lt`, `lte`, `eq` | `gt` = greater than | | `input:target` | Column containing ground truth labels | `target` (resolved via column binding) | | `input:dimension` | Protected attribute to slice by | `gender`, `age` (resolved via [Column Binding](/guides/column-binding/)) | | `input:prediction` | Column containing model predictions (model audits) | `prediction` | ## 3. Run Your Custom Policy [Section titled “3. Run Your Custom Policy”](#3-run-your-custom-policy) Now, let’s run the audit with *your* policy file. Copy-paste this code block: ```python import venturalitica as vl from venturalitica.quickstart import load_sample # 1. Load the German Credit Dataset (built-in sample) data = load_sample("loan") print(f"Dataset: {data.shape[0]} rows, {data.shape[1]} columns") # 2. Run Audit against your policy results = vl.enforce( data=data, target="class", # Ground truth column gender="Attribute9", # "Personal status and sex" -> gender age="Attribute13", # "Age in years" -> age policy="data_policy.oscal.yaml" ) # 3. Print results for r in results: status = "PASS" if r.passed else "FAIL" print(f" {r.control_id:<25} {r.actual_value:.3f} {r.operator} {r.threshold} {status}") ``` ### Expected output [Section titled “Expected output”](#expected-output) ```text Dataset: 1000 rows, 21 columns credit-data-imbalance 0.429 gt 0.2 PASS credit-data-bias 0.818 gt 0.8 PASS credit-age-disparate 0.286 gt 0.5 FAIL ``` Two controls pass, one fails. The age disparity ratio (0.286) is below the 0.5 threshold. ### The “Translation” Handshake [Section titled “The “Translation” Handshake”](#the-translation-handshake) Notice what just happened: * **Legal**: “Be fair (> 0.5).” — Defined in your YAML policy by the Compliance Officer. * **Dev**: “Column `Attribute13` means `age`.” — Defined in your Python call by the Engineer. This mapping is the **Handshake**. You bridge the gap between messy DataFrames and rigid legal requirements. This is how you implement **ISO 42001** without losing your mind in spreadsheets. ```text OSCAL Policy Python Code DataFrame +-----------+ +------------------+ +---------------+ | age | ----> | age="Attribute13"| ----> | Attribute13 | | gender | ----> | gender="Attr..9" | ----> | Attribute9 | | target | ----> | target="class" | ----> | class | +-----------+ +------------------+ +---------------+ ``` See [Column Binding](/guides/column-binding/) for the full resolution algorithm. ## 4. Visual Verification [Section titled “4. Visual Verification”](#4-visual-verification) The terminal output is evidence, but compliance needs professional reporting. Launch the local dashboard to visualize results: ```bash venturalitica ui ``` Navigate to the **Phase 3 (Verify & Evaluate)** tab. You will see: * Green checks for the two passing controls * A red flag for `credit-age-disparate` with the measured value (0.286) vs. threshold (0.5) * The trace JSON file is saved automatically as local evidence You have successfully prevented a non-compliant AI from reaching production by measuring risk against a verifiable standard. ## 5. Take Home Messages [Section titled “5. Take Home Messages”](#5-take-home-messages) 1. **Policy as Code**: Assurance is a `.yaml` file. It defines the **Controls** your system must pass. 2. **The Handshake**: You define the *Mapping* (`age`=`Attribute13`). The Officer defines the *Requirement* (`> 0.5`). Neither can act alone. 3. **Treatment starts with Detection**: The local failure is the signal necessary to start a formal ISO 42001 risk treatment plan. Don’t lower the threshold — fix the data. *** **Next Step**: The audit failed locally. How do we integrate this into an ML pipeline? **[Go to Level 2: The Integrator (MLOps)](/academy/level2-integrator/)**

# Level 2: The Integrator (GovOps & Visibility)

> Transform MLOps artifacts into Regulatory Evidence with a GovOps layer.

**Goal**: Transform MLOps artifacts into Regulatory Evidence with a **GovOps** layer. **Prerequisite**: [Level 1 (The Engineer)](/academy/level1-policy/) **Context**: Continuing with “The Project” (Loan Credit Scoring). *** ## 1. The Bottleneck: “It works on my machine” [Section titled “1. The Bottleneck: “It works on my machine””](#1-the-bottleneck-it-works-on-my-machine) In Level 1, you fixed the bias locally. But your manager denies it because they can’t see the proof. Emails with screenshots are **not compliance**. ## 2. The Solution: The GovOps Layer [Section titled “2. The Solution: The GovOps Layer”](#2-the-solution-the-govops-layer) In **GovOps** (Assurance over MLOps), we don’t treat compliance as a separate manual step. Instead, we use your existing MLOps infrastructure (MLflow, WandB) as an **Evidence Buffer** that automatically harvests the proof of safety during the training process. ### A. The Integration (Implicit Assurance) [Section titled “A. The Integration (Implicit Assurance)”](#a-the-integration-implicit-assurance) In a professional pipeline, assurance is a layer that wraps your training. Every time you train a model, you verify its compliance. Your experiment tracker now tracks two types of performance: **Accuracy** (Operational) and **Compliance** (Regulatory). * MLflow ```python import mlflow import venturalitica as vl from dataclasses import asdict from ucimlrepo import fetch_ucirepo from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression mlflow.set_tracking_uri("sqlite:///mlflow.db") mlflow.set_experiment("loan-credit-scoring") # 0. Load the UCI German Credit dataset (ID: 144) dataset = fetch_ucirepo(id=144) df = dataset.data.features.copy() df["class"] = dataset.data.targets X = df.select_dtypes(include=['number']).drop(columns=['class']) y = df['class'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 1. Start the GovOps Session (Implicitly captures 'Audit Trace') with mlflow.start_run(), vl.monitor("train_v1"): # 2. Pre-training Data Audit (Article 10) vl.enforce( data=df, target="class", gender="Attribute9", age="Attribute13", policy="data_policy.oscal.yaml" ) # 3. Train your model model = LogisticRegression(max_iter=1000) model.fit(X_train, y_train) # 4. Post-training Model Audit (Article 15: Human Oversight) # Download model_policy.oscal.yaml: # https://github.com/venturalitica/venturalitica-sdk-samples/blob/main/scenarios/loan-credit-scoring/policies/loan/model_policy.oscal.yaml test_df = X_test.copy() test_df["class"] = y_test test_df["prediction"] = model.predict(X_test) test_df["Attribute9"] = df.loc[X_test.index, "Attribute9"].values results = vl.enforce( data=test_df, target="class", prediction="prediction", gender="Attribute9", policy="model_policy.oscal.yaml" ) # 5. Log everything to the Evidence Buffer passed = all(r.passed for r in results) mlflow.log_metric("val_accuracy", model.score(X_test, y_test)) mlflow.log_metric("compliance_score", 1.0 if passed else 0.0) mlflow.log_dict([asdict(r) for r in results], "compliance_results.json") if not passed: # CRITICAL: Block the pipeline if the model is unethical raise ValueError("Model failed ISO 42001 compliance check. See audit trace.") ``` * Weights & Biases ```python import wandb import venturalitica as vl from dataclasses import asdict from ucimlrepo import fetch_ucirepo from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression wandb.init(project="loan-credit-scoring") # 0. Load the UCI German Credit dataset (ID: 144) dataset = fetch_ucirepo(id=144) df = dataset.data.features.copy() df["class"] = dataset.data.targets X = df.select_dtypes(include=['number']).drop(columns=['class']) y = df['class'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 1. Open a Monitor Context with vl.monitor("wandb_sync"): # 2. Pre-training Data Audit (Article 10) vl.enforce( data=df, target="class", gender="Attribute9", age="Attribute13", policy="data_policy.oscal.yaml" ) # 3. Train your model model = LogisticRegression(max_iter=1000) model.fit(X_train, y_train) # 4. Post-training Model Audit (Article 15) # Download model_policy.oscal.yaml: # https://github.com/venturalitica/venturalitica-sdk-samples/blob/main/scenarios/loan-credit-scoring/policies/loan/model_policy.oscal.yaml test_df = X_test.copy() test_df["class"] = y_test test_df["prediction"] = model.predict(X_test) test_df["Attribute9"] = df.loc[X_test.index, "Attribute9"].values audit = vl.enforce( data=test_df, target="class", prediction="prediction", gender="Attribute9", policy="model_policy.oscal.yaml" ) # 5. Log Compliance Artifacts artifact = wandb.Artifact('compliance-bundle', type='evidence') artifact.add_file(".venturalitica/results.json") artifact.add_file(".venturalitica/trace_wandb_sync.json") wandb.log_artifact(artifact) passed = all(r.passed for r in audit) wandb.log({"accuracy": model.score(X_test, y_test), "compliance": 1.0 if passed else 0.0}) if not passed: raise ValueError("Model rejected by GovOps policy.") ``` ### B. The Verification (Dashboard) [Section titled “B. The Verification (Dashboard)”](#b-the-verification-dashboard) Now that the code has run, let’s verify what we shipped. 1. **Run the UI**: ```bash venturalitica ui ``` 2. **Log Check**: Verify that `.venturalitica/results.json` exists (this is the default output of `enforce`). 3. **Navigate to “Policy Status”**: Confirm your “Risk Treatment” (the adjusted threshold) is recorded. **Key Insight**: “The report looks professional, and I didn’t write a single word of it.” ![Evidence Graph](/images/dashboard_overview.png) *** ## 3. Deep Dive: The Two-Policy Handshake (Art 10 vs 15) [Section titled “3. Deep Dive: The Two-Policy Handshake (Art 10 vs 15)”](#3-deep-dive-the-two-policy-handshake-art-10-vs-15) Professional GovOps requires a separation of concerns. You are now managing two distinct assurance layers: 1. **Level 1 (Article 10)**: Checked the **Raw Data** against `data_policy.yaml`. The goal was to prove the dataset itself was fair before wasting energy on training. 2. **Level 2 (Article 15)**: Checks the **Model Behavior** against `model_policy.yaml`. The goal is to prove the AI makes fair decisions in a “Glass Box” execution. | Stage | Variable Mapping | Policy File | Mandatory Requirement | | :-------------- | :---------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------- | | **Data Audit** | `target="class"` | [data\_policy.oscal.yaml](https://github.com/venturalitica/venturalitica-sdk-samples/blob/main/scenarios/loan-credit-scoring/policies/loan/data_policy.oscal.yaml) | **Article 10** (Data Assurance) | | **Model Audit** | `target="class", prediction="prediction"` | [model\_policy.oscal.yaml](https://github.com/venturalitica/venturalitica-sdk-samples/blob/main/scenarios/loan-credit-scoring/policies/loan/model_policy.oscal.yaml) | **Article 15** (Human Oversight) | This decoupling is the core of the **Handshake**. Even if the Law (`> 0.5`) stays the same, the *subject* of the law changes from **Data** to **Math**. ## 4. The Gate (CI/CD) [Section titled “4. The Gate (CI/CD)”](#4-the-gate-cicd) If `compliance_score == 0`, the build fails. GitLab CI / GitHub Actions can now block a deployment based on ethics, just like they block on syntax errors. *** ## 5. Take Home Messages [Section titled “5. Take Home Messages”](#5-take-home-messages) 1. **GovOps is Native**: Assurance isn’t an extra step; it’s a context manager (`vl.monitor`) around your training. 2. **Telemetry is Evidence**: RAM, CO2, and Trace results are not just for metrics — they fulfill **Article 15** oversight. 3. **Unified Trace**: `vl.monitor()` captures everything from hardware usage to AST code analysis in a single `.json` file. 4. **Zero Friction**: The Data Scientist continues to use MLflow/WandB, while the SDK harvests the evidence. *** ### References [Section titled “References”](#references) * **[API Reference](/reference/api/)** — `enforce()` and `monitor()` signatures * **[Policy Authoring](/guides/policy-authoring/)** — How to write OSCAL policies * **[Probes Reference](/reference/probes/)** — What `monitor()` captures automatically * **[Column Binding](/guides/column-binding/)** — How `gender="Attribute9"` works **[Next: Level 3 (The Auditor)](/academy/level3-auditor/)**

# Level 3: The Auditor (Glass Box Trace)

> Verify your policy visually and cryptographically using the Glass Box method.

**Goal**: Verify your policy visually and cryptographically using the **Glass Box** method. **Prerequisite**: [Level 2 (The Integrator)](/academy/level2-integrator/) *** ## 1. The Problem: “It passed, but can we trust the process?” [Section titled “1. The Problem: “It passed, but can we trust the process?””](#1-the-problem-it-passed-but-can-we-trust-the-process) In Level 2, you logged the compliance score. But for **High-Risk AI** (like Credit Scoring), metrics aren’t enough. An Auditor asks: *“Did you test on the real dataset, or did you filter out the rejected loans?”* and *“Can you prove this code was actually run?“* ## 2. The Solution: The “Glass Box” Trace [Section titled “2. The Solution: The “Glass Box” Trace”](#2-the-solution-the-glass-box-trace) Professional auditing requires more than just results — it requires **Provenance**. Venturalitica uses a `monitor()` context manager to record everything: * **The Code**: AST analysis of your script. * **The Data**: Row count and column schema. * **The Hardware**: Memory, CPU, and Carbon stats (Article 15). * **The Seal**: A cryptographic SHA-256 hash of the entire session. ### The Upgrade [Section titled “The Upgrade”](#the-upgrade) We continue working on the same project. No new setup required. ### Run with the Native Monitor [Section titled “Run with the Native Monitor”](#run-with-the-native-monitor) Wrap your execution in `vl.monitor()`. This context manager captures the “Handshake” between your code and the policy by harvesting both physical and logical metadata. ### Deep Dive: Glass Box vs Black Box [Section titled “Deep Dive: Glass Box vs Black Box”](#deep-dive-glass-box-vs-black-box) | Feature | Black Box (Standard) | **Glass Box (Venturalitica)** | | :-------- | :-------------------------- | :------------------------------------------------------------------ | | **Logic** | ”Trust me, I ran the code.” | **AST Analysis**: We record *which* function mapped code to policy. | | **Data** | ”Here is the CSV.” | **Fingerprint**: We record the SHA-256 of the dataset at runtime. | | **Scope** | Code | Code + Environment + Hardware Stats | ```python import venturalitica as vl from venturalitica.quickstart import load_sample # 1. Start the Multimodal Monitor (The Glass Box) with vl.monitor("loan_audit_v1"): # This block is now being watched by the Auditor df = load_sample("loan") # Download data_policy.oscal.yaml from: # https://github.com/venturalitica/venturalitica-sdk-samples/blob/main/scenarios/loan-credit-scoring/policies/loan/data_policy.oscal.yaml results = vl.enforce( data=df, target="class", gender="Attribute9", # Mapping gender age="Attribute13", # Mapping age policy="data_policy.oscal.yaml" ) # The session trace file is saved inside the run directory # and will prove NOT just the result, but HOW it was computed. # After the context manager exits, check the evidence directory: # .venturalitica/ # results.json <- Latest compliance results # runs/loan_audit_v1_<timestamp>/ # trace_loan_audit_v1.json <- Full execution trace # results.json <- Per-run compliance results # bom.json <- Software bill of materials ``` ## 3. The “Digital Seal” Verification [Section titled “3. The “Digital Seal” Verification”](#3-the-digital-seal-verification) After running the audit, launch the UI: ```bash venturalitica ui ``` Navigate to **“Article 13: Transparency”**. ### Finding the Evidence Hash [Section titled “Finding the Evidence Hash”](#finding-the-evidence-hash) Look for the **Evidence Hash** in the dashboard. `Evidence Hash: 89fbf...` This hash is your **“Digital Seal”**. If you change *one pixel* in the dataset or *one line* in the policy, this hash changes. You can now prove to a regulator exactly what happened during the audit. ## 4. The Compliance Map [Section titled “4. The Compliance Map”](#4-the-compliance-map) The Dashboard translates JSON evidence into the language of the **EU AI Act**. | Law | Dashboard Tab | What to Answer | | :--------- | :-------------- | :---------------------------------------- | | **Art 9** | Risk Management | ”Did we verify bias < 0.1?” (Your Policy) | | **Art 10** | Data Assurance | ”Is the training data representative?” | | **Art 13** | Transparency | ”What libraries (BOM) are we using?“ | ## 5. Take Home Messages [Section titled “5. Take Home Messages”](#5-take-home-messages) 1. **Don’t Trust, Verify**: The **Trace File** (captured automatically via `monitor()`) is the source of truth for the entire execution context. 2. **Glass Box Audit**: Compliance isn’t a “pass/fail” boolean; it’s a verifiable history of execution. 3. **Immutable Proof**: The Evidence Hash allows you to prove the integrity of the audit process. *** ### References [Section titled “References”](#references) * **[Probes Reference](/reference/probes/)** — Details on all 7 evidence probes * **[Dashboard Guide](/guides/dashboard/)** — Full walkthrough of the Dashboard phases * **[Full Lifecycle](/full-lifecycle/)** — End-to-end guide in one page **[Go to Level 4: The Architect](/academy/level4-annex-iv/)**

# Level 4: The Architect (Annex IV Generation)

> Automate the creation of 50+ page regulatory documents required by the EU AI Act.

**Goal**: Automate the creation of 50+ page regulatory documents. **Prerequisite**: [Level 3 (The Auditor)](/academy/level3-auditor/) *** ## 1. The Bottleneck: “Technical Documentation” [Section titled “1. The Bottleneck: “Technical Documentation””](#1-the-bottleneck-technical-documentation) According to **Article 11** and **Annex IV** of the EU AI Act, High-Risk systems (like **Credit Scoring**) require comprehensive Technical Documentation. Writing this manually takes **weeks**. ## 2. The Solution: Generative Compliance [Section titled “2. The Solution: Generative Compliance”](#2-the-solution-generative-compliance) We use your **Policies (Level 1 & 2)** and **Evidence (Level 2/3)** to prompt an LLM to draft the document for you. Venturalitica supports: * **Cloud**: Mistral (via API). * **Local**: Ollama (General purpose). * **Sovereign (NEW)**: **ALIA** (Spanish Native GGUF via Llama.cpp) - *Experimental*. ### The Upgrade [Section titled “The Upgrade”](#the-upgrade) We continue working on the “Loan Scoring” project. ### Run the High-Risk Audit [Section titled “Run the High-Risk Audit”](#run-the-high-risk-audit) Ensure you have run the collection steps: ```python import venturalitica as vl from venturalitica.quickstart import load_sample from sklearn.model_selection import train_test_split # 1. Load Data df = load_sample("loan") train_df = df.sample(frac=0.8, random_state=42) val_df = df.drop(train_df.index) # 2. Run the Article 10 (Data) & Article 15 (Model) Assurance Audit with vl.monitor("loan_annex_audit"): # 2.1 Verify Training Data (Art 10) # Download data_policy.oscal.yaml from: # https://github.com/venturalitica/venturalitica-sdk-samples/blob/main/scenarios/loan-credit-scoring/policies/loan/data_policy.oscal.yaml vl.enforce( data=train_df, target="class", gender="Attribute9", age="Attribute13", policy="data_policy.oscal.yaml" ) # 2.2 Verify Model Performance (Art 15) # Download model_policy.oscal.yaml from: # https://github.com/venturalitica/venturalitica-sdk-samples/blob/main/scenarios/loan-credit-scoring/policies/loan/model_policy.oscal.yaml vl.enforce( data=val_df.assign(prediction=val_df["class"]), # Simulated model target="class", prediction="prediction", gender="Attribute9", policy="model_policy.oscal.yaml" ) ``` ## 3. Generate the Document [Section titled “3. Generate the Document”](#3-generate-the-document) 1. Open the Dashboard: `venturalitica ui`. 2. Go to the **“Annex IV Generator”** tab. 3. Select Provider: **Cloud (Mistral)**, **Local (Ollama)**, or **Sovereign (ALIA - Experimental)**. 4. Click **“Generate Annex IV”**. ![Annex IV Generator](/images/annex_iv_generator.png) ### The Generation Process [Section titled “The Generation Process”](#the-generation-process) Watch the logs. The System is acting as a **Team of Agents**: 1. **Scanner**: Reads your `trace.json` (The Evidence). 2. **Planner**: Decides which sections of Annex IV apply to your specific model type. 3. **Writer**: Drafts “Section 2.c: Architecture” using the `summary()` from your actual Python code. 4. **Critic**: Reviews the draft against the ISO 42001 standard. **Result**: A markdown file (`Annex_IV.md`) that cites your specific accuracy scores (e.g., `Demographic Parity: 0.92`) as proof of safety. ## 4. Selecting your LLM [Section titled “4. Selecting your LLM”](#4-selecting-your-llm) | Feature | Cloud (Mistral API) | Local (Ollama) | Sovereign (ALIA - Experimental) | | :-------------- | :------------------------ | :---------------- | :------------------------------ | | **Privacy** | Encrypted Transport | 100% Offline | Hardware Locked | | **Sovereignty** | Hosted in EU | Generic | **Spanish Native** | | **Speed** | Fast (Large Model) | Slower | **Slow (Experimental)** | | **Use Case** | Final High-Quality Polish | Iterative Testing | **Research Only** | We currently offer **ALIA** as an experimental feature for organizations piloting Spanish-native sovereign AI. Caution **Experimental Feature & Hardware Requirements** ALIA is a 40B parameter model. It is marked as **EXPERIMENTAL** and requires significant hardware resources: * **RAM/VRAM**: \~41GB required (Q8 quantization). * **GPU**: A high-end GPU (e.g., RTX 3090/4090 with 24GB+) is recommended for usable speeds. * **Performance**: On consumer hardware or smaller GPUs (like RTX 2000), inference will effectively run on CPU and be very slow. ## 5. Export to PDF [Section titled “5. Export to PDF”](#5-export-to-pdf) By default, we generate `Annex_IV.md` (Markdown) for version control. To convert this to a regulatory-grade PDF: **Python (mdpdf):** ```bash uv pip install mdpdf uv run mdpdf Annex_IV.md ``` **Pandoc (Advanced):** ```bash pandoc Annex_IV.md -o Annex_IV.pdf --toc --pdf-engine=xelatex ``` ## 6. Take Home Messages [Section titled “6. Take Home Messages”](#6-take-home-messages) 1. **Documentation is a Function**: `f(Evidence) -> Document`. Never write what you can generate. 2. **LiveTrace**: If your accuracy drops tomorrow, regenerate the document. It will reflect the *current* state, preventing “Documentation Drift”. 3. **The Full Loop**: You have gone from Code -> Policy (L1) -> Ops (L2) -> Evidence (L3) -> Legal Document (L4). *** ### Congratulations! [Section titled “Congratulations!”](#congratulations) You have completed the **Venturalitica Academy**. You are now ready to integrate this into your own CI/CD pipeline. ### References [Section titled “References”](#references) * **[Full Lifecycle](/full-lifecycle/)** — The entire flow in one copy-paste page * **[Dashboard Guide](/guides/dashboard/)** — Detailed Dashboard and Phase 4 walkthrough * **[Metrics Reference](/reference/metrics/)** — All 35+ metrics available for policies * **[Policy Authoring](/guides/policy-authoring/)** — Write custom OSCAL policies

# Full Lifecycle: Zero to Annex IV

> A single-page walkthrough of the entire Venturalitica compliance lifecycle using the Loan Credit Scoring scenario.

A single-page walkthrough of the entire Venturalitica compliance lifecycle using the **Loan Credit Scoring** scenario. Each step is copy-paste ready. **Time**: \~15 minutes. **Prerequisites**: Python 3.11+, `pip install venturalitica`. *** ## Overview [Section titled “Overview”](#overview) ```text ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐ │ Install │───►│ Write │───►│ Audit │───►│ Audit │───►│ Generate │ │ │ │ Policy │ │ Data │ │ Model │ │ Report │ │ pip │ │ OSCAL │ │ enforce()│ │ enforce()│ │ Annex IV │ └──────────┘ └──────────┘ └──────────┘ └──────────┘ └──────────┘ Art 9 Art 10 Art 15 Art 11 ``` EU AI Act mapping: | Step | Article | Purpose | | :-------------- | :------------------------------------------ | :---------------------------------------- | | Write Policy | Art 9 (Risk Management) | Define controls for identified risks | | Audit Data | Art 10 (Data Governance) | Verify training data quality and fairness | | Audit Model | Art 15 (Accuracy & Robustness) | Verify model behavior post-training | | Generate Report | Art 11 / Annex IV (Technical Documentation) | Produce regulatory documentation | *** ## Step 1: Install and Verify [Section titled “Step 1: Install and Verify”](#step-1-install-and-verify) ```bash pip install venturalitica ``` Quick smoke test: ```python import venturalitica as vl results = vl.quickstart("loan") # Expected: 2 PASS, 1 FAIL (age disparity) ``` *** ## Step 2: Write Policy Files [Section titled “Step 2: Write Policy Files”](#step-2-write-policy-files) Create two OSCAL policy files in your project directory. ### `data_policy.oscal.yaml` — Pre-Training Controls [Section titled “data\_policy.oscal.yaml — Pre-Training Controls”](#data_policyoscalyaml--pre-training-controls) ```yaml assessment-plan: metadata: title: Credit Risk Assessment Policy (German Credit) version: "1.1" control-implementations: - description: Credit Scoring Fairness Controls implemented-requirements: - control-id: credit-data-imbalance description: "Minority class >= 20% of dataset" props: - name: metric_key value: class_imbalance - name: threshold value: "0.2" - name: operator value: gt - name: "input:target" value: target - control-id: credit-data-bias description: "Gender disparate impact follows Four-Fifths Rule" props: - name: metric_key value: disparate_impact - name: threshold value: "0.8" - name: operator value: gt - name: "input:target" value: target - name: "input:dimension" value: gender - control-id: credit-age-disparate description: "Age disparate impact ratio > 0.5" props: - name: metric_key value: disparate_impact - name: threshold value: "0.50" - name: operator value: gt - name: "input:target" value: target - name: "input:dimension" value: age ``` ### `model_policy.oscal.yaml` — Post-Training Controls [Section titled “model\_policy.oscal.yaml — Post-Training Controls”](#model_policyoscalyaml--post-training-controls) ```yaml assessment-plan: metadata: title: "Article 15: Model Accuracy and Fairness" version: "1.0" control-implementations: - description: Model Assurance Controls implemented-requirements: - control-id: model-accuracy description: "Model accuracy >= 70%" props: - name: metric_key value: accuracy_score - name: threshold value: "0.70" - name: operator value: gte - name: "input:target" value: target - name: "input:prediction" value: prediction - control-id: model-gender-fairness description: "Demographic parity difference < 0.10" props: - name: metric_key value: demographic_parity_diff - name: threshold value: "0.10" - name: operator value: lt - name: "input:target" value: target - name: "input:prediction" value: prediction - name: "input:dimension" value: gender ``` See [Policy Authoring Guide](/guides/policy-authoring/) for the full format reference. *** ## Step 3: Audit the Training Data (Article 10) [Section titled “Step 3: Audit the Training Data (Article 10)”](#step-3-audit-the-training-data-article-10) ```python import venturalitica as vl from venturalitica.quickstart import load_sample # Load the German Credit dataset df = load_sample("loan") # Audit data quality and fairness BEFORE training data_results = vl.enforce( data=df, target="class", gender="Attribute9", # "Personal status and sex" age="Attribute13", # "Age in years" policy="data_policy.oscal.yaml" ) for r in data_results: status = "PASS" if r.passed else "FAIL" print(f" {r.control_id:<25} {r.actual_value:.3f} {r.operator} {r.threshold} {status}") ``` Expected output: ```text credit-data-imbalance 0.429 gt 0.2 PASS credit-data-bias 0.818 gt 0.8 PASS credit-age-disparate 0.286 gt 0.5 FAIL ``` The age disparity fails. In a real project you would address this before training. For this walkthrough we continue to demonstrate the full flow. *** ## Step 4: Train and Audit the Model (Article 15) [Section titled “Step 4: Train and Audit the Model (Article 15)”](#step-4-train-and-audit-the-model-article-15) ```python from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split # Prepare features (numeric only for simplicity) X = df.select_dtypes(include=["number"]).drop(columns=["class"]) y = df["class"] X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42 ) # Train model = LogisticRegression(max_iter=1000) model.fit(X_train, y_train) # Predict on test set predictions = model.predict(X_test) test_df = X_test.copy() test_df["class"] = y_test test_df["prediction"] = predictions test_df["Attribute9"] = df.loc[X_test.index, "Attribute9"].values # Restore protected attribute # Audit model behavior model_results = vl.enforce( data=test_df, target="class", prediction="prediction", gender="Attribute9", policy="model_policy.oscal.yaml" ) for r in model_results: status = "PASS" if r.passed else "FAIL" print(f" {r.control_id:<25} {r.actual_value:.3f} {r.operator} {r.threshold} {status}") ``` *** ## Step 5: Wrap with Evidence Collection [Section titled “Step 5: Wrap with Evidence Collection”](#step-5-wrap-with-evidence-collection) In production, wrap the entire pipeline in `vl.monitor()` to capture evidence automatically: ```python import venturalitica as vl from venturalitica.quickstart import load_sample from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split df = load_sample("loan") with vl.monitor("loan_full_audit"): # --- Article 10: Data Audit --- data_results = vl.enforce( data=df, target="class", gender="Attribute9", age="Attribute13", policy="data_policy.oscal.yaml" ) # --- Train --- X = df.select_dtypes(include=["number"]).drop(columns=["class"]) y = df["class"] X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42 ) model = LogisticRegression(max_iter=1000) model.fit(X_train, y_train) # --- Article 15: Model Audit --- test_df = X_test.copy() test_df["class"] = y_test test_df["prediction"] = model.predict(X_test) test_df["Attribute9"] = df.loc[X_test.index, "Attribute9"].values # Restore protected attribute model_results = vl.enforce( data=test_df, target="class", prediction="prediction", gender="Attribute9", policy="model_policy.oscal.yaml" ) # Evidence is now saved in .venturalitica/ # - trace_loan_full_audit.json (execution trace) # - results.json (compliance results) ``` The `monitor()` context manager automatically captures: * **Hardware probe**: CPU, RAM, GPU info * **Carbon probe**: Energy consumption estimate * **BOM probe**: Software bill of materials (installed packages) * **Artifact probe**: SHA-256 hashes of data and policy files * **Trace probe**: AST analysis of executed code See [Probes Reference](/reference/probes/) for details on each probe. *** ## Step 6: Visualize and Generate Annex IV [Section titled “Step 6: Visualize and Generate Annex IV”](#step-6-visualize-and-generate-annex-iv) ### Launch the Dashboard [Section titled “Launch the Dashboard”](#launch-the-dashboard) ```bash venturalitica ui ``` The Dashboard presents results across 4 phases: | Phase | What You See | | :----------------------- | :---------------------------------------------- | | **1. System Identity** | Project name, version, AI system classification | | **2. Risk Policy** | Your OSCAL controls with pass/fail status | | **3. Verify & Evaluate** | Metric values, charts, evidence hashes | | **4. Technical Report** | Annex IV document generator | ### Generate Annex IV [Section titled “Generate Annex IV”](#generate-annex-iv) 1. Navigate to Phase 4 in the Dashboard 2. Select your LLM provider (Mistral API, Ollama, or ALIA) 3. Click **Generate Annex IV** 4. The system reads your trace files and drafts a regulatory document Output: `Annex_IV.md` — a structured document citing your actual metric values as proof of compliance. Convert to PDF: ```bash pip install mdpdf mdpdf Annex_IV.md ``` *** ## Files Produced [Section titled “Files Produced”](#files-produced) After running the full lifecycle, your project contains: ```text my-project/ data_policy.oscal.yaml # Step 2: Data governance controls model_policy.oscal.yaml # Step 2: Model assurance controls .venturalitica/ results.json # Step 3-4: Compliance results (latest) runs/ latest -> loan_full_audit_… # Symlink to the most recent run loan_full_audit_<timestamp>/ trace_loan_full_audit.json # Step 5: Full execution trace results.json # Per-run compliance results bom.json # Software bill of materials Annex_IV.md # Step 6: Generated documentation ``` *** ## What’s Next [Section titled “What’s Next”](#whats-next) * **[Metrics Reference](/reference/metrics/)** — All 35+ available metrics * **[Academy Level 1](/academy/level1-policy/)** — Deep dive into policy writing * **[Column Binding](/guides/column-binding/)** — How abstract names map to columns * **[Dashboard Guide](/guides/dashboard/)** — Detailed Dashboard walkthrough

# Column Binding

> Map abstract policy names (gender, age) to actual DataFrame columns via synonym-based resolution.

Venturalitica uses a **synonym-based column binding** system to map abstract concepts (like “gender” or “age”) to actual DataFrame column names (like “Attribute9” or “Attribute13”). This decouples your OSCAL policies from specific dataset schemas. *** ## How It Works [Section titled “How It Works”](#how-it-works) When you call `vl.enforce()`, the SDK resolves column names in three steps: 1. **Explicit parameters**: Check if `target` or `prediction` values exist as column names 2. **Synonym discovery**: Look up the column name in `COLUMN_SYNONYMS` 3. **Lowercase fallback**: Try a case-insensitive match ### Example [Section titled “Example”](#example) ```python vl.enforce( data=df, target="class", # Explicit: looks for 'class' column gender="Attribute9", # Attribute mapping: 'gender' -> 'Attribute9' age="Attribute13", # Attribute mapping: 'age' -> 'Attribute13' policy="data_policy.oscal.yaml" ) ``` In the OSCAL policy, controls reference abstract names like `gender` and `age` via `input:dimension`. The SDK resolves these to actual columns at runtime. *** ## Synonym Dictionary [Section titled “Synonym Dictionary”](#synonym-dictionary) The built-in `COLUMN_SYNONYMS` dictionary maps semantic roles to known column name variants: | Role | Known Synonyms | | :----------- | :---------------------------------------------------------------------------------------------- | | `gender` | `sex`, `gender`, `sexo`, `Attribute9` | | `age` | `age`, `age_group`, `edad`, `Attribute13` | | `race` | `race`, `ethnicity`, `raza` | | `target` | `target`, `class`, `label`, `y`, `true_label`, `ground_truth`, `approved`, `default`, `outcome` | | `prediction` | `prediction`, `pred`, `y_pred`, `predictions`, `score`, `proba`, `output` | | `dimension` | `sex`, `gender`, `age`, `race`, `Attribute9`, `Attribute13` | ### Auto-Discovery [Section titled “Auto-Discovery”](#auto-discovery) If you do not explicitly pass a column mapping, the SDK automatically discovers columns by scanning your DataFrame for known synonyms: ```python # These are equivalent for the UCI German Credit dataset: vl.enforce(data=df, target="class", gender="Attribute9") vl.enforce(data=df) # Auto-discovers 'class' as target, 'Attribute9' as gender ``` *** ## Resolution Functions [Section titled “Resolution Functions”](#resolution-functions) ### `discover_column(requested, context_mapping, data, synonyms)` [Section titled “discover\_column(requested, context\_mapping, data, synonyms)”](#discover_columnrequested-context_mapping-data-synonyms) Discovers a single column using this priority order: 1. Check `context_mapping` (explicit user-provided mapping) 2. Check if `requested` is a direct column name 3. Search `COLUMN_SYNONYMS` for a matching group 4. Lowercase fallback 5. Return `"MISSING"` if not found ### `resolve_col_names(val, data, synonyms)` [Section titled “resolve\_col\_names(val, data, synonyms)”](#resolve_col_namesval-data-synonyms) Resolves one or more column names from a string or list: * Supports comma-separated strings: `"target, gender"` * Supports lists: `["target", "gender"]` * Returns a list of resolved column names *** ## The Policy-Code Handshake [Section titled “The Policy-Code Handshake”](#the-policy-code-handshake) Column binding bridges the gap between **legal requirements** (OSCAL policies) and **technical reality** (messy DataFrames): ```plaintext OSCAL Policy Python Code DataFrame +--------------+ enforce() +-----------+ binding +------------+ | dimension: | ------------> | gender= | ------------> | Attribute9 | | gender | | "Attr..." | | (actual | | threshold: | | | | column) | | > 0.8 | | | | | +--------------+ +-----------+ +------------+ ``` This design means: * **Compliance Officers** write policies using abstract names (`gender`, `age`) * **Engineers** provide the mapping to technical column names once * **The same policy** works across different datasets with different schemas *** ## Multilingual Support [Section titled “Multilingual Support”](#multilingual-support) The synonym dictionary includes Spanish translations: * `gender` = `sexo` * `age` = `edad` * `race` = `raza` This allows datasets with Spanish column headers to be used without explicit mapping. *** ## Custom Synonyms [Section titled “Custom Synonyms”](#custom-synonyms) You can extend the synonym dictionary programmatically: ```python from venturalitica.binding import COLUMN_SYNONYMS # Add a custom synonym for your dataset COLUMN_SYNONYMS["income"] = ["income", "salary", "wage", "earnings", "ingreso"] ``` Or pass a custom synonym dictionary to the resolution functions: ```python from venturalitica.binding import resolve_col_names custom_synonyms = { "risk_score": ["risk", "risk_score", "risk_level", "riesgo"], } resolved = resolve_col_names("risk_score", df, synonyms=custom_synonyms) ```

# Compliance Mapping: EU AI Act & ISO 42001

> How Venturalitica SDK capabilities map to EU AI Act articles and ISO/IEC 42001 controls.

This document maps Venturalitica SDK capabilities to the **EU AI Act** articles and **ISO/IEC 42001** controls relevant to high-risk AI systems. *** ## EU AI Act Article Mapping [Section titled “EU AI Act Article Mapping”](#eu-ai-act-article-mapping) ### Article 9: Risk Management System [Section titled “Article 9: Risk Management System”](#article-9-risk-management-system) **Requirement**: Establish a risk management system throughout the AI system lifecycle. | SDK Capability | How It Fulfills Art 9 | | :----------------- | :------------------------------------------------- | | OSCAL policy files | Risk controls codified as machine-readable rules | | `enforce()` | Automated risk evaluation against defined controls | | Dashboard Phase 1 | System identity and risk context documentation | | Dashboard Phase 2 | Visual policy editor for risk control definition | **Example**: Define a risk control that checks age disparity: ```yaml - control-id: credit-age-disparate description: "Age disparate impact ratio > 0.5" props: - name: metric_key value: disparate_impact - name: threshold value: "0.50" - name: operator value: gt - name: "input:dimension" value: age ``` *** ### Article 10: Data and Data Governance [Section titled “Article 10: Data and Data Governance”](#article-10-data-and-data-governance) **Requirement**: Training, validation, and testing data sets shall be relevant, representative, free of errors, and complete. | SDK Capability | How It Fulfills Art 10 | | :------------------------------------------ | :-------------------------------------------------------- | | `class_imbalance` metric | Checks minority class representation | | `disparate_impact` metric | Checks group-level selection rates | | `data_completeness` metric | Measures missing values | | `k_anonymity`, `l_diversity`, `t_closeness` | Privacy-preserving data quality | | Data policy pattern | Separate `data_policy.oscal.yaml` for pre-training checks | **Key metrics for Art 10**: | Metric | Art 10 Clause | Purpose | | :------------------------ | :------------------------ | :------------------------------------- | | `class_imbalance` | 10.3 (representative) | Ensure minority classes are not erased | | `disparate_impact` | 10.2.f (bias examination) | Four-Fifths Rule across groups | | `data_completeness` | 10.3 (free of errors) | Detect missing data | | `group_min_positive_rate` | 10.3 (representative) | Minimum positive rate per group | *** ### Article 11: Technical Documentation (Annex IV) [Section titled “Article 11: Technical Documentation (Annex IV)”](#article-11-technical-documentation-annex-iv) **Requirement**: Technical documentation shall be drawn up before the AI system is placed on the market. | SDK Capability | How It Fulfills Art 11 | | :---------------------- | :--------------------------------------------------- | | `monitor()` trace files | Automatic evidence collection (code, data, hardware) | | Evidence hash (SHA-256) | Cryptographic proof of execution integrity | | Dashboard Phase 4 | Annex IV document generation via LLM | | BOM probe | Software bill of materials for reproducibility | **Evidence files produced**: ```text .venturalitica/ trace_<session>.json # Execution trace with AST analysis results.json # Compliance results per control Annex_IV.md # Generated documentation (Phase 4) ``` *** ### Article 13: Transparency [Section titled “Article 13: Transparency”](#article-13-transparency) **Requirement**: High-risk AI systems shall be designed to ensure their operation is sufficiently transparent. | SDK Capability | How It Fulfills Art 13 | | :------------------ | :------------------------------------- | | Glass Box method | Full execution trace, not just results | | AST code analysis | Records which functions were called | | Data fingerprinting | SHA-256 of input data at runtime | | Artifact probe | Hash of policy files used | *** ### Article 15: Accuracy, Robustness, and Cybersecurity [Section titled “Article 15: Accuracy, Robustness, and Cybersecurity”](#article-15-accuracy-robustness-and-cybersecurity) **Requirement**: High-risk AI systems shall achieve an appropriate level of accuracy, robustness, and cybersecurity. | SDK Capability | How It Fulfills Art 15 | | :-------------------------------------------------------------- | :---------------------------------------------------------- | | `accuracy_score`, `precision_score`, `recall_score`, `f1_score` | Performance metrics | | `demographic_parity_diff`, `equal_opportunity_diff` | Fairness metrics on model predictions | | Model policy pattern | Separate `model_policy.oscal.yaml` for post-training checks | | Hardware probe | CPU, RAM, GPU monitoring for robustness evidence | | Carbon probe | Energy consumption tracking | **Key metrics for Art 15**: | Metric | Art 15 Clause | Purpose | | :------------------------ | :------------------------ | :------------------------------ | | `accuracy_score` | 15.1 (accuracy) | Model achieves minimum accuracy | | `demographic_parity_diff` | 15.3 (non-discrimination) | Prediction rates are fair | | `equalized_odds_ratio` | 15.3 (non-discrimination) | Error rates are equitable | | `counterfactual_fairness` | 15.3 (non-discrimination) | Causal fairness analysis | *** ## ISO/IEC 42001 Mapping [Section titled “ISO/IEC 42001 Mapping”](#isoiec-42001-mapping) ISO 42001 defines an **AI Management System (AIMS)** framework. Venturalitica maps to the following control areas: ### Annex A Controls [Section titled “Annex A Controls”](#annex-a-controls) | ISO 42001 Control | Description | SDK Mapping | | :------------------------------------------ | :----------------------------------- | :----------------------------------------------------------------- | | **A.2** AI Policy | Organization-level AI policy | OSCAL policy files define machine-readable policies | | **A.4** AI Risk Assessment | Identify and assess AI risks | `enforce()` evaluates controls; Dashboard Phase 2 visualizes risks | | **A.5** AI Risk Treatment | Implement controls to mitigate risks | OSCAL controls with thresholds implement risk treatment | | **A.6** AI System Impact Assessment | Assess impact on individuals/groups | Fairness metrics (`disparate_impact`, `demographic_parity_diff`) | | **A.7** Data for AI Systems | Data quality management | Data policy pattern + data quality metrics | | **A.8** AI System Documentation | Document AI system lifecycle | `monitor()` traces + Dashboard Phase 4 (Annex IV generation) | | **A.9** AI System Performance | Monitor system performance | Performance metrics + `monitor()` evidence collection | | **A.10** Third-party and Customer Relations | Supply chain transparency | BOM probe captures all dependencies | ### Clause 6: Planning [Section titled “Clause 6: Planning”](#clause-6-planning) | ISO 42001 Clause | Description | SDK Mapping | | :------------------ | :-------------------------------- | :---------------------------------------------------------- | | 6.1 Risk assessment | Determine risks and opportunities | OSCAL policy defines measurable risk thresholds | | 6.2 AI objectives | Set measurable objectives | Each OSCAL control is a measurable objective with pass/fail | ### Clause 9: Performance Evaluation [Section titled “Clause 9: Performance Evaluation”](#clause-9-performance-evaluation) | ISO 42001 Clause | Description | SDK Mapping | | :-------------------- | :---------------------------- | :------------------------------------------------------ | | 9.1 Monitoring | Monitor AI system performance | `enforce()` + `monitor()` provide continuous evaluation | | 9.2 Internal audit | Audit the AIMS | Evidence traces provide audit trail | | 9.3 Management review | Review AIMS effectiveness | Dashboard provides visual review interface | ### Clause 10: Improvement [Section titled “Clause 10: Improvement”](#clause-10-improvement) | ISO 42001 Clause | Description | SDK Mapping | | :------------------------- | :---------------------- | :----------------------------------------------------------- | | 10.1 Nonconformity | Handle control failures | `enforce()` flags failures; `strict=True` raises exceptions | | 10.2 Continual improvement | Improve the AIMS | Version policies, re-run audits, track improvement over time | *** ## The Two-Policy Pattern and Regulatory Mapping [Section titled “The Two-Policy Pattern and Regulatory Mapping”](#the-two-policy-pattern-and-regulatory-mapping) Venturalitica’s two-policy pattern maps directly to the regulatory structure: ```text Regulation Policy File SDK Function Phase ----------- --------------- ---------------- ----- Art 10 (Data) --> data_policy.oscal.yaml --> enforce(target=...) Pre-training Art 15 (Model) --> model_policy.oscal.yaml-> enforce(prediction=..) Post-training Art 11 (Docs) --> (generated) --> Dashboard Phase 4 Reporting Art 9 (Risk) --> (both policies) --> All of the above Continuous ``` *** ## Audit Evidence Chain [Section titled “Audit Evidence Chain”](#audit-evidence-chain) A complete compliance audit produces the following evidence chain: | Evidence | EU AI Act | ISO 42001 | File | | :---------------------- | :---------------- | :-------- | :--------------------------- | | Policy definition | Art 9 | A.2, A.5 | `*.oscal.yaml` | | Data quality results | Art 10 | A.7 | `results.json` | | Model fairness results | Art 15 | A.6, A.9 | `results.json` | | Execution trace | Art 13 | A.8 | `trace_*.json` | | Software BOM | Art 15 | A.10 | `trace_*.json` (BOM section) | | Hardware/carbon metrics | Art 15 | A.9 | `trace_*.json` (probes) | | Technical documentation | Art 11 / Annex IV | A.8 | `Annex_IV.md` | *** ## Related [Section titled “Related”](#related) * **[Full Lifecycle](/full-lifecycle/)** — Step-by-step guide implementing this mapping * **[Policy Authoring](/guides/policy-authoring/)** — Write OSCAL controls for each article * **[Metrics Reference](/reference/metrics/)** — All available metrics by category * **[Probes Reference](/reference/probes/)** — Evidence probes mapped to EU AI Act articles * **[Dashboard Guide](/guides/dashboard/)** — 4-phase workflow aligned with regulatory requirements

# Glass Box Dashboard

> 4-phase AI Assurance workspace for EU AI Act: Identity, Policy, Verify, Report.

The **Venturalítica Dashboard** is your local AI Assurance workspace. It guides you through 4 phases of the EU AI Act evidence collection lifecycle without leaving your terminal. Caution This tool **assists with evidence gathering** and does not constitute legal advice or guarantee regulatory compliance. ## Launch [Section titled “Launch”](#launch) ```bash venturalitica ui ``` Or with uv: ```bash uv run venturalitica ui ``` The dashboard opens at `http://localhost:8501` in your default browser. *** ## Dashboard Architecture [Section titled “Dashboard Architecture”](#dashboard-architecture) The dashboard follows a **4-phase Assurance Journey** mapped to EU AI Act requirements: ```plaintext Home (AI Assurance — Evidence Collection) | +-- Phase 1: System Identity (Annex IV.1) | +-- Phase 2: Risk Policy (Articles 9-10) | +-- Phase 3: Verify & Evaluate (Articles 11-15) | | | +-- Transparency Feed | +-- Technical Integrity | +-- Policy Enforcement | +-- Phase 4: Technical Report (Annex IV) ``` **Phase gating** is enforced: Phase 2 requires Phase 1 evidence, Phase 3 requires Phase 2, and Phase 4 requires Phase 3. *** ## Home: AI Assurance — Evidence Collection [Section titled “Home: AI Assurance — Evidence Collection”](#home-ai-assurance--evidence-collection) The home screen presents 4 steps as a progress dashboard. Each step shows its evidence status: | Step | Status Check | Description | | :----------------- | :---------------------------------------- | :--------------------------------------- | | 1. Define System | `system_description.yaml` exists | System identity and hardware description | | 2. Define Policies | `model_policy.oscal.yaml` exists | OSCAL risk and data governance policies | | 3. Review Evidence | `results.json` or `trace_*.json` exists | Telemetry, traces, and metric validation | | 4. Generate Report | `venturalitica_technical_doc.json` exists | Generated Annex IV technical file | Click any step card to navigate directly to that phase. ![Dashboard Home — AI Assurance Evidence Collection](/images/dashboard_overview.png) *** ## Phase 1: System Identity [Section titled “Phase 1: System Identity”](#phase-1-system-identity) **EU AI Act:** Annex IV.1 (General Description of the AI System) Define the “ground truth” of your AI system using the **System Identity Editor**. This creates `system_description.yaml` with: * **System name and version** * **Intended purpose** (e.g., “Credit scoring for loan applications”) * **Provider information** * **Hardware description** (compute resources used) * **Interaction description** (how users interact with the system) The editor provides a structured form. All fields map directly to Annex IV.1 requirements. ![Phase 1 — System Identity Editor](/images/phase1_system_identity.png) *** ## Phase 2: Risk Policy [Section titled “Phase 2: Risk Policy”](#phase-2-risk-policy) **EU AI Act:** Articles 9 (Risk Management) and 10 (Data Governance) The **Policy Editor** lets you create and edit OSCAL policy files visually — this is the **Compliance-as-Code** step. It generates `assessment-plan` format OSCAL YAML with: * **Model Policy** (`model_policy.oscal.yaml`): Fairness and performance controls for model behavior * **Data Policy** (`data_policy.oscal.yaml`): Data quality and privacy controls for training data ### Policy Editor Features [Section titled “Policy Editor Features”](#policy-editor-features) * Add controls with metric selection from the full registry * Set thresholds and comparison operators * Map protected attributes (dimension binding) * Preview the generated OSCAL YAML * Save directly to your project directory ![Phase 2 — Risk Policy Editor](/images/phase2_risk_policy.png) *** ## Phase 3: Verify & Evaluate [Section titled “Phase 3: Verify & Evaluate”](#phase-3-verify--evaluate) **EU AI Act:** Articles 11-15 (Technical Documentation, Record-Keeping, Transparency, Human Oversight, Accuracy) This phase requires evidence from running `vl.enforce()` and `vl.monitor()`. Select an evidence session from the sidebar to inspect. ![Phase 3 — Verify & Evaluate](/images/phase3_verify_evaluate.png) ### Session Selector [Section titled “Session Selector”](#session-selector) The sidebar shows all available evidence sessions: * **Global / History**: Aggregated results from `.venturalitica/results.json` * **Named sessions**: Individual `vl.monitor("session_name")` runs with their own trace files ### Tab: Transparency Feed [Section titled “Tab: Transparency Feed”](#tab-transparency-feed) Maps to **Article 13** (Transparency). Shows: * Software Bill of Materials (SBOM) — all Python dependencies with versions * Code context — AST analysis of the script that generated evidence * Runtime metadata — timestamps, duration, success/failure status ### Tab: Technical Integrity [Section titled “Tab: Technical Integrity”](#tab-technical-integrity) Maps to **Article 15** (Accuracy, Robustness, Cybersecurity). Shows: * Environment fingerprint (SHA-256 hash) * Integrity drift detection (did the environment change during execution?) * Hardware telemetry (peak RAM, CPU count) * Carbon emissions (if CodeCarbon is installed) ### Tab: Policy Enforcement [Section titled “Tab: Policy Enforcement”](#tab-policy-enforcement) Maps to **Article 9** (Risk Management). Shows: * Per-control assurance results with pass/fail status * Actual metric values vs. policy thresholds * Visual breakdown of which controls passed and which failed * Assurance score summary *** ## Phase 4: Technical Report [Section titled “Phase 4: Technical Report”](#phase-4-technical-report) **EU AI Act:** Article 11 and Annex IV (Technical Documentation) The **Annex IV Generator** produces the comprehensive technical documentation required for High-Risk AI systems. It combines: ![Phase 4 — Annex IV Generator](/images/annex_iv_generator.png) * **Phase 1 data**: System identity from `system_description.yaml` * **Phase 2 data**: Risk policies from OSCAL files * **Phase 3 data**: Evidence from enforcement results and traces ### LLM Provider Selection [Section titled “LLM Provider Selection”](#llm-provider-selection) | Provider | Privacy | Sovereignty | Speed | Use Case | | :------------------ | :------------------ | :------------- | :----- | :---------------- | | Cloud (Mistral API) | Encrypted transport | EU-hosted | Fast | Final polish | | Local (Ollama) | 100% offline | Generic | Slower | Iterative testing | | Sovereign (ALIA) | Hardware locked | Spanish native | Slow | Research only | Caution **ALIA Requirements**: ALIA is a 40B parameter model requiring \~41GB RAM/VRAM. It is experimental. ### Generation Process [Section titled “Generation Process”](#generation-process) 1. **Scanner**: Reads trace files and evidence 2. **Planner**: Determines which Annex IV sections apply 3. **Writer**: Drafts each section citing specific metric values 4. **Critic**: Reviews the draft against ISO 42001 ### Output [Section titled “Output”](#output) The generator produces: * `venturalitica_technical_doc.json` — structured data * `Annex_IV.md` — human-readable markdown document Convert to PDF: ```bash # Simple pip install mdpdf && mdpdf Annex_IV.md # Advanced pandoc Annex_IV.md -o Annex_IV.pdf --toc --pdf-engine=xelatex ``` *** ## Project Context [Section titled “Project Context”](#project-context) The dashboard operates on your **current working directory**. It reads: | File | Purpose | | :--------------------------------- | :--------------------------------- | | `system_description.yaml` | Phase 1 system identity | | `model_policy.oscal.yaml` | Phase 2 model policy | | `data_policy.oscal.yaml` | Phase 2 data policy | | `.venturalitica/results.json` | Phase 3 enforcement results | | `.venturalitica/trace_*.json` | Phase 3 execution traces | | `.venturalitica/bom.json` | Phase 3 software bill of materials | | `venturalitica_technical_doc.json` | Phase 4 generated documentation | Run your `vl.enforce()` and `vl.monitor()` calls from the same directory where you launch `venturalitica ui`. *** ## Keyboard Shortcuts [Section titled “Keyboard Shortcuts”](#keyboard-shortcuts) The dashboard uses Streamlit. Standard Streamlit shortcuts apply: * `R` — Rerun the app * `C` — Clear cache * Settings menu (top-right hamburger) for theme switching

# Experimental Features

> CLI commands and features that depend on the Venturalitica SaaS platform (not yet publicly available).

This page documents CLI commands and features that are currently experimental. Their API may change or be removed in future releases. Caution **Experimental**: These features depend on the Venturalitica SaaS platform, which is not yet publicly available. They are included in the SDK for early adopters and internal testing. *** ## CLI: `login` [Section titled “CLI: login”](#cli-login) Authenticate with the Venturalitica SaaS platform. ```bash venturalitica login ``` Stores credentials locally for use by `pull` and `push` commands. Authentication is required before using any SaaS-connected features. *** ## CLI: `pull` [Section titled “CLI: pull”](#cli-pull) Pull OSCAL policies and system configuration from the SaaS platform to your local project. ```bash venturalitica pull ``` ### What It Downloads [Section titled “What It Downloads”](#what-it-downloads) | File | Description | | :------------------------ | :------------------------------------ | | `model_policy.oscal.yaml` | Model fairness and performance policy | | `data_policy.oscal.yaml` | Data quality and privacy policy | | `system_description.yaml` | Annex IV.1 system identity fields | ### Behavior [Section titled “Behavior”](#behavior) * Authenticates using stored credentials from `login` * Fetches policies from `/api/pull?format=oscal` * Writes files to the current working directory * Reports bound and unbound risks from the SaaS platform *** ## CLI: `push` [Section titled “CLI: push”](#cli-push) Push local compliance evidence to the Venturalitica SaaS platform. ```bash venturalitica push ``` Uploads enforcement results and trace files from `.venturalitica/` to the cloud for team visibility and audit trails. *** ## CLI: `ui` (Stable) [Section titled “CLI: ui (Stable)”](#cli-ui-stable) Launch the local Glass Box Dashboard. This command is **stable** and fully documented. ```bash venturalitica ui ``` See [Dashboard Guide](/guides/dashboard/) for full documentation. *** ## Feature Status Matrix [Section titled “Feature Status Matrix”](#feature-status-matrix) | Feature | Status | Depends On | | :-------------------- | :----------- | :------------ | | `venturalitica ui` | Stable | Local only | | `venturalitica login` | Experimental | SaaS platform | | `venturalitica pull` | Experimental | SaaS platform | | `venturalitica push` | Experimental | SaaS platform | | `vl.enforce()` | Stable | Local only | | `vl.monitor()` | Stable | Local only | | `vl.quickstart()` | Stable | Local only | | `vl.wrap()` | Experimental | Local only | *** ## SDK ↔ SaaS Data Flow [Section titled “SDK ↔ SaaS Data Flow”](#sdk--saas-data-flow) The experimental CLI commands enable a handshake between your local environment and the SaaS platform: ```text Developer Workstation Venturalitica SaaS ════════════════════ ═══════════════════ venturalitica login ───────────────► Auth (API key) ◄───────────── Token stored locally venturalitica pull ───────────────► GET /api/pull?format=oscal ◄───────────── OSCAL policies + system_description vl.enforce(policy=…) ──► local (no network call) vl.monitor(…) ──► local .venturalitica/trace_*.json venturalitica push ───────────────► POST /api/push ◄───────────── Evidence archived in cloud ``` All enforcement and monitoring runs **locally** — the SaaS platform is only used for policy distribution and evidence archival. *** ## Providing Feedback [Section titled “Providing Feedback”](#providing-feedback) If you are testing experimental features, report issues at [GitHub Issues](https://github.com/venturalitica/venturalitica-sdk/issues/new).

# Policy Authoring Guide

> Write OSCAL policies from scratch: assessment-plan format, controls, thresholds, and the two-policy pattern.

This guide explains how to write OSCAL policy files for Venturalitica. Policies define the fairness, performance, privacy, and data quality controls your AI system must pass. *** ## The Canonical Format: `assessment-plan` [Section titled “The Canonical Format: assessment-plan”](#the-canonical-format-assessment-plan) Venturalitica uses the **OSCAL `assessment-plan`** format as its canonical policy format. While the SDK loader supports multiple OSCAL document types (`catalog`, `system-security-plan`, `component-definition`, `profile`), you should write new policies in `assessment-plan` format. *** ## Minimal Policy [Section titled “Minimal Policy”](#minimal-policy) The simplest valid policy has one control: ```yaml assessment-plan: metadata: title: "My First Policy" control-implementations: - description: "Fairness Controls" implemented-requirements: - control-id: bias-check description: "Disparate impact must satisfy the Four-Fifths Rule" props: - name: metric_key value: disparate_impact - name: threshold value: "0.8" - name: operator value: ">" - name: "input:dimension" value: gender ``` Enforce it: ```python import venturalitica as vl results = vl.enforce( data=df, target="class", gender="Attribute9", policy="my_policy.oscal.yaml" ) ``` *** ## Control Anatomy [Section titled “Control Anatomy”](#control-anatomy) Each control (an `implemented-requirement`) has these properties: | Property | Required | Description | | :------------ | :------- | :--------------------------------------- | | `control-id` | Yes | Unique identifier for this control | | `description` | No | Human-readable description | | `props` | Yes | List of key-value properties (see below) | ### Props Reference [Section titled “Props Reference”](#props-reference) | Prop Name | Required | Description | Example | | :----------------- | :------- | :--------------------------------------- | :--------------------------------------------------- | | `metric_key` | Yes | Registry key of the metric to compute | `disparate_impact` | | `threshold` | Yes | Numeric threshold value (as string) | `"0.8"` | | `operator` | Yes | Comparison operator | `">"`, `"<"`, `">="`, `"<="`, `"=="`, `"gt"`, `"lt"` | | `input:dimension` | Depends | Protected attribute for fairness metrics | `gender`, `age` | | `input:target` | No | Override target column for this control | `class` | | `input:prediction` | No | Override prediction column | `y_pred` | *** ## The Two-Policy Pattern [Section titled “The Two-Policy Pattern”](#the-two-policy-pattern) Professional compliance separates data and model audits into two policies: ### Data Policy (Article 10) [Section titled “Data Policy (Article 10)”](#data-policy-article-10) Checks the training data **before** model training: ```yaml assessment-plan: metadata: title: "Article 10: Data Assurance" control-implementations: - description: "Data Quality & Fairness" implemented-requirements: - control-id: data-imbalance description: "Minority class must be > 20%" props: - name: metric_key value: class_imbalance - name: threshold value: "0.2" - name: operator value: ">" - control-id: data-gender-bias description: "Gender disparate impact > 0.8 (Four-Fifths Rule)" props: - name: metric_key value: disparate_impact - name: "input:dimension" value: gender - name: threshold value: "0.8" - name: operator value: ">" - control-id: data-age-bias description: "Age disparity > 0.5" props: - name: metric_key value: disparate_impact - name: "input:dimension" value: age - name: threshold value: "0.5" - name: operator value: ">" ``` ### Model Policy (Article 15) [Section titled “Model Policy (Article 15)”](#model-policy-article-15) Checks model predictions **after** training: ```yaml assessment-plan: metadata: title: "Article 15: Model Assurance" control-implementations: - description: "Model Performance & Fairness" implemented-requirements: - control-id: model-accuracy description: "Model accuracy >= 80%" props: - name: metric_key value: accuracy_score - name: threshold value: "0.80" - name: operator value: ">=" - control-id: model-fairness description: "Demographic parity difference < 0.10" props: - name: metric_key value: demographic_parity_diff - name: "input:dimension" value: gender - name: threshold value: "0.10" - name: operator value: "<" ``` ### Enforcing Both [Section titled “Enforcing Both”](#enforcing-both) The recommended pattern is two separate calls — one per audit stage: ```python # Pre-training: audit the data vl.enforce(data=train_df, target="class", gender="Attribute9", policy="data_policy.oscal.yaml") # Post-training: audit the model vl.enforce(data=test_df, target="class", prediction="y_pred", gender="Attribute9", policy="model_policy.oscal.yaml") ``` You can also pass a list of policies in a single call. Controls whose required inputs are not provided (e.g., `prediction` for model policies) will be **silently skipped**: ```python # Only data-policy controls will run here because # 'prediction' is not provided — model-policy controls are skipped. vl.enforce( data=df, target="class", gender="Attribute9", policy=["data_policy.oscal.yaml", "model_policy.oscal.yaml"] ) # To evaluate both policies, supply all required inputs: vl.enforce( data=test_df, target="class", prediction="y_pred", gender="Attribute9", policy=["data_policy.oscal.yaml", "model_policy.oscal.yaml"] ) ``` *** ## Available Metrics [Section titled “Available Metrics”](#available-metrics) Any metric registered in `METRIC_REGISTRY` can be used as a `metric_key`. See [Metrics Reference](/reference/metrics/) for the full list. Common ones: | Category | Metric Key | Typical Operator | Typical Threshold | | :--------------- | :------------------------ | :--------------- | :---------------- | | **Data Quality** | `disparate_impact` | `>` | `0.8` | | **Data Quality** | `class_imbalance` | `>` | `0.2` | | **Performance** | `accuracy_score` | `>=` | `0.80` | | **Performance** | `f1_score` | `>=` | `0.75` | | **Fairness** | `demographic_parity_diff` | `<` | `0.10` | | **Fairness** | `equalized_odds_ratio` | `<` | `0.20` | | **Privacy** | `k_anonymity` | `>=` | `5` | *** ## Dimension Mapping [Section titled “Dimension Mapping”](#dimension-mapping) The `input:dimension` prop tells the SDK which protected attribute to analyze. The value is an abstract name that gets resolved via [Column Binding](/guides/column-binding/): ```yaml # In your policy: - name: "input:dimension" value: gender # Abstract name # In your Python: vl.enforce(data=df, gender="Attribute9") # Maps 'gender' -> 'Attribute9' ``` *** ## Multiple Control Implementations [Section titled “Multiple Control Implementations”](#multiple-control-implementations) You can group controls logically: ```yaml assessment-plan: metadata: title: "Comprehensive AI Assurance Policy" control-implementations: - description: "Data Quality Controls (Article 10)" implemented-requirements: - control-id: dq-001 # ... - control-id: dq-002 # ... - description: "Fairness Controls (Article 9)" implemented-requirements: - control-id: fair-001 # ... - description: "Privacy Controls (GDPR)" implemented-requirements: - control-id: priv-001 # ... ``` *** ## Visual Authoring [Section titled “Visual Authoring”](#visual-authoring) The Dashboard Policy Editor provides a visual interface for creating policies: 1. Run `venturalitica ui` 2. Navigate to **Phase 2: Risk Policy** 3. Use the form to add controls, select metrics, and set thresholds 4. The editor generates `assessment-plan` OSCAL YAML and saves it to your project See [Dashboard Guide](/guides/dashboard/) for details. *** ## File Naming Convention [Section titled “File Naming Convention”](#file-naming-convention) | File | Purpose | | :------------------------ | :----------------------------------------------------- | | `data_policy.oscal.yaml` | Pre-training data audit controls | | `model_policy.oscal.yaml` | Post-training model audit controls | | `risks.oscal.yaml` | Combined quickstart policy (used by `vl.quickstart()`) | The `.oscal.yaml` extension is a convention, not a requirement. The SDK loads any `.yaml` file. *** ## Supported OSCAL Formats [Section titled “Supported OSCAL Formats”](#supported-oscal-formats) While `assessment-plan` is canonical, the loader also accepts: | Format | Support Level | Notes | | :--------------------- | :------------ | :----------------------------------------- | | `assessment-plan` | Primary | Canonical format, Dashboard generates this | | `catalog` | Supported | Used in some advanced samples | | `system-security-plan` | Supported | Used by SaaS `pull` command | | `component-definition` | Supported | Standard OSCAL component format | | `profile` | Supported | OSCAL profile format | | Flat YAML list | Fallback | Emergency format for simple lists | Caution **SSP Format Inconsistency**: The CLI `pull` command currently downloads policies in `system-security-plan` format. These load correctly but differ from the `assessment-plan` format generated by the Dashboard. A future release will unify this.

# 60-Second Quickstart

> Your first bias audit in under 60 seconds.

**Goal**: Your first bias audit in under 60 seconds. *** ## The Fundamentals: From Risk to Code [Section titled “The Fundamentals: From Risk to Code”](#the-fundamentals-from-risk-to-code) Building High-Risk AI requires a fundamental shift in how we approach testing. It is no longer enough to check for technical accuracy (e.g., F1 Score); we must now mathematically prove that the system respects fundamental rights, such as non-discrimination or data quality, as mandated by the **EU AI Act**. Venturalitica automates this by treating “Assurance” as a dependency. Instead of vague legal requirements, you define strict policies (OSCAL) that your model must pass before it can be deployed. This turns compliance into a deterministic engineering problem. **The Translation Layer:** 1. **Fundamental Risk**: “The model must not discriminate against protected groups” (Art 9). 2. **Policy Control**: “Disparate Impact Ratio must be > 0.8”. 3. **Code Assertion**: `assert calculated_metric > 0.8`. When you run `quickstart()`, you are technically running a **Unit Test for Ethics**. *** ## Step 1: Install [Section titled “Step 1: Install”](#step-1-install) ```bash pip install venturalitica ``` *** ## Step 2: Run Your First Audit [Section titled “Step 2: Run Your First Audit”](#step-2-run-your-first-audit) ```python import venturalitica as vl vl.quickstart('loan') ``` **Output:** ```text [Venturalítica] Scenario: Fairness Audit loan_scoring_v2 [Venturalítica] Loaded: UCI Dataset #144 (1000 samples) CONTROL DESCRIPTION ACTUAL LIMIT RESULT ──────────────────────────────────────────────────────────────────────────────────────────────── credit-data-imbalance Data Quality 0.429 > 0.2 PASS credit-data-bias Disparate impact 0.818 > 0.8 PASS credit-age-disparate Age disparity 0.286 > 0.5 FAIL ──────────────────────────────────────────────────────────────────────────────────────────────── Audit Summary: VIOLATION | 2/3 controls passed ``` ## Step 3: What’s Happening Under the Hood [Section titled “Step 3: What’s Happening Under the Hood”](#step-3-whats-happening-under-the-hood) The `quickstart()` function is a wrapper that performs the full compliance lifecycle in one go: 1. **Downloads Data**: Fetches the UCI German Credit dataset. 2. **Loads Policy**: Uses a built-in OSCAL policy that defines fairness rules (thresholds, metrics, protected attributes). 3. **Enforces**: Runs the audit (`vl.enforce`). 4. **Records**: Captures the evidence (`trace.json`) for the dashboard. Here’s what the equivalent “manual” flow looks like. In the [Full Lifecycle](/full-lifecycle/) guide you will write your own OSCAL policies and run this yourself: ```python from ucimlrepo import fetch_ucirepo import venturalitica as vl # 1. Load Data (The "Risk Source") dataset = fetch_ucirepo(id=144) df = dataset.data.features df['class'] = dataset.data.targets # 2. Define the Policy (The "Law") # quickstart() uses a built-in policy dict. # In real projects, you write your own OSCAL YAML file. # See the Full Lifecycle guide for a copy-paste example. # 3. Run the Audit (The "Test") # This automatically generates the Evidence Bill of Materials (BOM) with vl.monitor("manual_audit"): vl.enforce( data=df, target="class", # The outcome (True/False) gender="Attribute9", # Protected Group A age="Attribute13", # Protected Group B policy="data_policy.oscal.yaml" # Your OSCAL policy file ) ``` ### The Policy Logic [Section titled “The Policy Logic”](#the-policy-logic) The OSCAL policy is the bridge between law and code. It tells the SDK *what* to check so you don’t have to hardcode it. ```yaml # ... inside your OSCAL policy YAML ... - control-id: credit-data-bias description: "Disparate impact ratio must be > 0.8 (80% rule)" props: - name: metric_key value: disparate_impact # <--- The Python Function to call - name: threshold value: "0.8" # <--- The Limit to enforce - name: operator value: ">" # <--- The Logic (> 0.8) - name: "input:dimension" value: gender # <--- Maps to "Attribute9" ``` This design decouples **Assurance** (the policy file) from **Engineering** (the python code). *** ## Why This Matters [Section titled “Why This Matters”](#why-this-matters) Without this mechanism, your AI model is a legal “Black Box”: * **Liability**: You cannot prove you checked for bias *before* deployment (Art 9). * **Fragility**: Compliance is a manual checklist, easily forgotten or skipped. * **Opacity**: Auditors cannot see the link between your code and the law. By running `quickstart()`, you have just generated an immutable **Compliance Artifact**. Even if the laws change, your evidence remains. ## Step 4: The “Glass Box” Dashboard [Section titled “Step 4: The “Glass Box” Dashboard”](#step-4-the-glass-box-dashboard) Now that we have the evidence (the “Black Box” recording), let’s inspect it in the **Regulatory Map**. ```bash venturalitica ui ``` Navigate through the **Compliance Map** tabs: * **Article 9 (Risk)**: See the failed `credit-age-disparate` control. This is your technical evidence of “Risk Monitoring”. * **Article 10 (Data)**: See the data distribution and quality checks. * **Article 13 (Transparency)**: Review the “Transparency Feed” to see your Python dependencies (BOM). *** ## Step 5: Generate Documentation (Annex IV) [Section titled “Step 5: Generate Documentation (Annex IV)”](#step-5-generate-documentation-annex-iv) The final step is to turn this evidence into a legal document. 1. In the Dashboard, go to the **“Generation”** tab. 2. Select **“English”** (or Spanish/Catalan/Euskera). 3. Click **“Generate Annex IV”**. Venturalitica will draft a technical document that references your specific run: > *“As evidenced in `trace_quickstart_loan.json`, the system was audited against **\[OSCAL Policy: Credit Scoring Fairness]**. A deviation was detected in Age Disparity (0.36), identifying a potential risk of bias…”* ### References [Section titled “References”](#references) * **Policy Used**: [`loan/risks.oscal.yaml`](https://github.com/venturalitica/venturalitica-sdk-samples/blob/main/policies/loan/risks.oscal.yaml) * **Legal Basis**: * [EU AI Act Article 9 (Risk Management)](https://artificialintelligenceact.eu/article/9/) * [EU AI Act Article 11 (Technical Documentation)](https://artificialintelligenceact.eu/article/11/) ## What’s Next? [Section titled “What’s Next?”](#whats-next) * **[API Reference](/reference/api/)** — Full function signatures and parameters * **[Policy Authoring Guide](/guides/policy-authoring/)** — Write your own OSCAL policies from scratch * **[Metrics Reference](/reference/metrics/)** — All 35+ available metrics * **[Venturalitica Academy](/academy/index/)** — Guided learning path from Engineer to Architect

# API Reference

> Complete API reference for enforce(), monitor(), wrap(), quickstart(), and PolicyManager.

Venturalitica exposes five public symbols. This page documents their exact signatures and behavior as of v0.5.0. *** ## Core Functions [Section titled “Core Functions”](#core-functions) ### `quickstart(scenario, verbose=True)` [Section titled “quickstart(scenario, verbose=True)”](#quickstartscenario-verbosetrue) Run a pre-configured bias audit demo on a standard dataset. This is the fastest way to see the SDK in action. ```python import venturalitica as vl results = vl.quickstart("loan") ``` | Parameter | Type | Default | Description | | :--------- | :----- | :----------- | :---------------------------------------------------------- | | `scenario` | `str` | *(required)* | Predefined scenario name: `"loan"`, `"hiring"`, `"health"`. | | `verbose` | `bool` | `True` | Print the structured compliance table to the console. | **Returns:** `List[ComplianceResult]` *** ### `enforce()` [Section titled “enforce()”](#enforce) The main entry point for auditing datasets and models against OSCAL policies. ```python def enforce( data=None, metrics=None, policy="risks.oscal.yaml", target="target", prediction="prediction", strict=False, **attributes, ) -> List[ComplianceResult] ``` | Parameter | Type | Default | Description | | :------------- | :--------------------------- | :------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | | `data` | `DataFrame` or `None` | `None` | Pandas DataFrame containing features, targets, and optionally predictions. | | `metrics` | `Dict[str, float]` or `None` | `None` | Pre-computed metrics dict. Use this when you have already calculated your metrics externally. | | `policy` | `str`, `Path`, or `List` | `"risks.oscal.yaml"` | Path to one or more OSCAL policy files. Pass a list to enforce multiple policies in a single call. | | `target` | `str` | `"target"` | Name of the column containing ground truth labels. | | `prediction` | `str` | `"prediction"` | Name of the column containing model predictions. | | `strict` | `bool` | `False` | If `True`, missing metrics, unbound variables, and calculation errors raise exceptions instead of being skipped. Auto-enabled when `CI=true` or `VENTURALITICA_STRICT=true`. | | `**attributes` | keyword args | | Mappings for protected variables and dimensions. For example: `gender="Attribute9"`, `age="Attribute13"`. | **Returns:** `List[ComplianceResult]` #### Two Modes of Operation [Section titled “Two Modes of Operation”](#two-modes-of-operation) **Mode 1: DataFrame-based** (most common). Pass a DataFrame and let the SDK compute metrics automatically: ```python results = vl.enforce( data=df, target="class", prediction="prediction", gender="Attribute9", # maps abstract 'gender' -> column 'Attribute9' age="Attribute13", # maps abstract 'age' -> column 'Attribute13' policy="data_policy.oscal.yaml", ) ``` **Mode 2: Pre-computed metrics**. Pass a dict of already-calculated values: ```python results = vl.enforce( metrics={"accuracy_score": 0.92, "demographic_parity_diff": 0.07}, policy="model_policy.oscal.yaml", ) ``` #### Column Binding [Section titled “Column Binding”](#column-binding) When using DataFrame mode, the SDK resolves column names through a synonym system (see [Column Binding](/guides/column-binding/)): * `target` and `prediction` are resolved first via explicit parameters, then via synonym discovery. * `**attributes` (e.g., `gender="Attribute9"`) are passed directly to metric functions as the `dimension` parameter. * If a column is not found, the SDK falls back to lowercase matching. #### Results Caching [Section titled “Results Caching”](#results-caching) `enforce()` automatically caches results to `.venturalitica/results.json` and, if inside a `monitor()` session, to the session-specific evidence directory. Run `venturalitica ui` to visualize cached results. #### Multiple Policies [Section titled “Multiple Policies”](#multiple-policies) Pass a list to enforce several policies in one call: ```python results = vl.enforce( data=df, target="class", policy=["data_policy.oscal.yaml", "model_policy.oscal.yaml"], gender="Attribute9", ) ``` *** ### `monitor(name, label=None, inputs=None, outputs=None)` [Section titled “monitor(name, label=None, inputs=None, outputs=None)”](#monitorname-labelnone-inputsnone-outputsnone) A context manager that records multimodal telemetry during training or evaluation. Captures hardware, carbon, security, and audit evidence automatically. ```python @contextmanager def monitor( name="Training Task", label=None, inputs=None, outputs=None, ) ``` | Parameter | Type | Default | Description | | :-------- | :-------------------- | :---------------- | :-------------------------------------------------------------------------- | | `name` | `str` | `"Training Task"` | Human-readable name for this monitoring session. Used in trace filenames. | | `label` | `str` or `None` | `None` | Optional label for categorization (e.g., `"pre-training"`, `"validation"`). | | `inputs` | `List[str]` or `None` | `None` | Paths to input artifacts (datasets, configs) for data lineage tracking. | | `outputs` | `List[str]` or `None` | `None` | Paths to output artifacts (models, plots) for lineage tracking. | #### Usage [Section titled “Usage”](#usage) ```python with vl.monitor("credit_model_v1"): model.fit(X_train, y_train) vl.enforce(data=df, policy="policy.oscal.yaml", target="class") ``` #### Probes Collected [Section titled “Probes Collected”](#probes-collected) `monitor()` initializes 7 probes automatically. See [Probes Reference](/reference/probes/) for details. | Probe | What It Captures | EU AI Act Article | | :--------------- | :------------------------------------------------ | :---------------- | | `IntegrityProbe` | SHA-256 environment fingerprint, drift detection | Art. 15 | | `HardwareProbe` | Peak RAM, CPU count | Art. 15 | | `CarbonProbe` | CO2 emissions via CodeCarbon | Art. 15 | | `BOMProbe` | Software Bill of Materials (SBOM) | Art. 13 | | `ArtifactProbe` | Input/output data lineage | Art. 10 | | `HandshakeProbe` | Whether `enforce()` was called inside the session | Art. 9 | | `TraceProbe` | AST code analysis, timestamps, call context | Art. 11 | Evidence is saved to `.venturalitica/` or a session-specific directory. *** ### `wrap(model, policy)` — Experimental [Section titled “wrap(model, policy) — Experimental”](#wrapmodel-policy--experimental) Caution **PREVIEW**: This function is experimental and its API may change. Transparently audit your model during Scikit-Learn standard workflows by hooking into `.fit()` and `.predict()`. | Parameter | Type | Description | | :-------- | :------- | :--------------------------------------------------- | | `model` | `object` | Any Scikit-learn compatible classifier or regressor. | | `policy` | `str` | Path to the OSCAL policy for evaluation. | **Returns:** `AssuranceWrapper` (preserves the original model API: `.fit()`, `.predict()`, etc.) ```python wrapped = vl.wrap(LogisticRegression(), policy="model_policy.oscal.yaml") wrapped.fit(X_train, y_train) preds = wrapped.predict(X_test) # Audit runs automatically ``` *** ## Utility Class [Section titled “Utility Class”](#utility-class) ### `PolicyManager` [Section titled “PolicyManager”](#policymanager) Programmatic access to OSCAL policy loading and manipulation. ```python from venturalitica import PolicyManager ``` *** ## Data Types [Section titled “Data Types”](#data-types) ### `ComplianceResult` [Section titled “ComplianceResult”](#complianceresult) Every call to `enforce()` returns a list of `ComplianceResult` dataclass instances: | Field | Type | Description | | :------------ | :------ | :-------------------------------------------------------------------------- | | `control_id` | `str` | The control identifier from the policy (e.g., `"credit-data-bias"`). | | `description` | `str` | Human-readable description of the control. | | `metric_key` | `str` | The metric function used (e.g., `"disparate_impact"`). | | `actual` | `float` | The computed metric value. | | `threshold` | `float` | The policy-defined threshold. | | `operator` | `str` | Comparison operator (`">"`, `"<"`, `">="`, `"<="`, `"=="`, `"gt"`, `"lt"`). | | `passed` | `bool` | Whether the control passed. | ```python for r in results: print(f"{r.control_id}: {r.actual:.3f} {'PASS' if r.passed else 'FAIL'}") ```

# Metrics Reference

> All 35+ metrics across 7 categories: performance, fairness, data quality, privacy, and causal fairness.

Venturalitica ships with 35+ metrics organized into 7 categories. Each metric is registered in `METRIC_REGISTRY` and can be referenced by its key in OSCAL policy files. *** ## Metric Categories at a Glance [Section titled “Metric Categories at a Glance”](#metric-categories-at-a-glance) | Category | Metrics | Description | | :------------------------- | :------ | :------------------------------------------------------- | | **Performance** | 4 | Standard ML accuracy, precision, recall, F1 | | **Fairness (Traditional)** | 2 | Demographic parity, equal opportunity | | **Fairness (Alternative)** | 2 | Equalized odds, predictive parity | | **Multiclass Fairness** | 7 | Fairness metrics for multi-class classification | | **Data Quality** | 4 | Disparate impact, class imbalance, completeness | | **Privacy** | 4 | k-anonymity, l-diversity, t-closeness, data minimization | | **Causal Fairness** | 4 | Counterfactual, path decomposition, awareness | *** ## Performance Metrics [Section titled “Performance Metrics”](#performance-metrics) | Registry Key | Function | Description | | :---------------- | :--------------- | :------------------------------------ | | `accuracy_score` | `calc_accuracy` | Overall classification accuracy | | `precision_score` | `calc_precision` | Positive predictive value | | `recall_score` | `calc_recall` | Sensitivity / true positive rate | | `f1_score` | `calc_f1` | Harmonic mean of precision and recall | **Usage in policy:** ```yaml - control-id: model-accuracy props: - name: metric_key value: accuracy_score - name: threshold value: "0.80" - name: operator value: ">=" ``` *** ## Fairness Metrics (Traditional) [Section titled “Fairness Metrics (Traditional)”](#fairness-metrics-traditional) These are the most commonly used fairness measures for binary classification. ### `demographic_parity_diff` [Section titled “demographic\_parity\_diff”](#demographic_parity_diff) Measures the difference in positive prediction rates between protected groups. * **Formula:** |P(Y=1|A=a) - P(Y=1|A=b)| * **Ideal value:** 0.0 * **Typical threshold:** < 0.10 * **Requires:** `dimension` (protected attribute column) ### `equal_opportunity_diff` [Section titled “equal\_opportunity\_diff”](#equal_opportunity_diff) Measures the difference in true positive rates (TPR) between groups. * **Formula:** |TPR\_a - TPR\_b| * **Ideal value:** 0.0 * **Typical threshold:** < 0.10 * **Requires:** `dimension`, `target`, `prediction` *** ## Fairness Metrics (Alternative) [Section titled “Fairness Metrics (Alternative)”](#fairness-metrics-alternative) ### `equalized_odds_ratio` [Section titled “equalized\_odds\_ratio”](#equalized_odds_ratio) Ensures both TPR and FPR are equal across groups. Stricter than equal opportunity. * **Formula:** |TPR\_a - TPR\_b| + |FPR\_a - FPR\_b| * **Ideal value:** 0.0 * **Typical threshold:** < 0.20 * **Requires:** `dimension`, `target`, `prediction` * **Reference:** [Hardt et al. 2016](https://arxiv.org/abs/1610.02413) ### `predictive_parity` [Section titled “predictive\_parity”](#predictive_parity) Measures precision equality across groups. When a positive prediction is made, it should be equally reliable regardless of group membership. * **Formula:** |Precision\_a - Precision\_b| * **Ideal value:** 0.0 * **Typical threshold:** < 0.10 * **Requires:** `dimension`, `target`, `prediction` * **Reference:** [Corbett-Davies et al. 2017](https://arxiv.org/abs/1708.09055) *** ## Multiclass Fairness Metrics [Section titled “Multiclass Fairness Metrics”](#multiclass-fairness-metrics) For multi-class classification tasks. See [Multiclass Fairness](/reference/multiclass-fairness/) for detailed usage. | Registry Key | Description | | :--------------------------------------- | :----------------------------------------- | | `multiclass_demographic_parity` | Demographic parity extended to multi-class | | `multiclass_equal_opportunity` | Equal opportunity per class | | `multiclass_confusion_metrics` | Per-group confusion matrix analysis | | `weighted_demographic_parity_multiclass` | Class-weighted demographic parity | | `macro_equal_opportunity_multiclass` | Macro-averaged equal opportunity | | `micro_equalized_odds_multiclass` | Micro-averaged equalized odds | | `predictive_parity_multiclass` | Predictive parity per class | *** ## Data Quality Metrics [Section titled “Data Quality Metrics”](#data-quality-metrics) | Registry Key | Description | Typical Threshold | | :------------------------ | :------------------------------------ | :---------------- | | `disparate_impact` | Four-Fifths Rule ratio between groups | > 0.80 | | `class_imbalance` | Minority class proportion | > 0.20 | | `group_min_positive_rate` | Minimum positive rate across groups | > 0.10 | | `data_completeness` | Proportion of non-null values | > 0.95 | **Usage example (loan scenario):** ```yaml - control-id: credit-data-bias description: "Disparate impact ratio must satisfy the Four-Fifths Rule" props: - name: metric_key value: disparate_impact - name: "input:dimension" value: gender - name: operator value: ">" - name: threshold value: "0.8" ``` *** ## Privacy Metrics [Section titled “Privacy Metrics”](#privacy-metrics) GDPR-aligned privacy measures from `venturalitica.assurance.privacy`. ### `k_anonymity` [Section titled “k\_anonymity”](#k_anonymity) Minimum group size when quasi-identifiers are known. Prevents re-identification. * **Formula:** min(|group|) where groups are defined by quasi-identifiers * **Ideal value:** >= 5 (GDPR recommendation) * **Requires:** quasi-identifier columns ```python from venturalitica.metrics.privacy import calc_k_anonymity k = calc_k_anonymity(df, quasi_identifiers=["age", "gender", "zipcode"]) assert k >= 5, "GDPR recommends k >= 5" ``` ### `l_diversity` [Section titled “l\_diversity”](#l_diversity) Minimum distinct values of a sensitive attribute per quasi-identifier group. * **Formula:** min(distinct values in sensitive\_attribute per QI group) * **Ideal value:** >= 2 ### `t_closeness` [Section titled “t\_closeness”](#t_closeness) Maximum distribution difference between groups using Earth Mover Distance. * **Formula:** max(EMD between group distributions) * **Ideal value:** < 0.15 * **Reference:** [Li et al. 2007](https://en.wikipedia.org/wiki/T-closeness) ### `data_minimization` [Section titled “data\_minimization”](#data_minimization) GDPR Article 5 compliance — proportion of non-sensitive columns. * **Formula:** (total\_columns - sensitive\_columns) / total\_columns * **Ideal value:** >= 0.70 ```python from venturalitica.metrics.privacy import calc_data_minimization_score score = calc_data_minimization_score( df, sensitive_columns=["age", "income", "health_status"] ) ``` *** ## Causal Fairness Metrics [Section titled “Causal Fairness Metrics”](#causal-fairness-metrics) Advanced metrics from `venturalitica.assurance.causal`. | Registry Key | Description | | :--------------------------- | :-------------------------------------------------------------------- | | `path_decomposition` | Decomposes causal paths to identify direct vs indirect discrimination | | `counterfactual_fairness` | Tests whether changing a protected attribute would change the outcome | | `fairness_through_awareness` | Ensures similar individuals receive similar predictions | | `causal_fairness_diagnostic` | Comprehensive diagnostic combining multiple causal tests | *** ## LLM & Benchmark Aliases [Section titled “LLM & Benchmark Aliases”](#llm--benchmark-aliases) These are convenience aliases that use `calc_mean` internally: | Registry Key | Use Case | | :--------------------------- | :----------------------------------------- | | `bias_score` | General bias scoring for LLM outputs | | `stereotype_preference_rate` | Stereotype detection in generated text | | `category_bias_score` | Per-category bias in benchmark evaluations | *** ## ESG / Financial QA Metrics [Section titled “ESG / Financial QA Metrics”](#esg--financial-qa-metrics) Specialized metrics for ESG report analysis: | Registry Key | Description | | :---------------------------- | :---------------------------------------- | | `classification_distribution` | Distribution of ESG classifications | | `report_coverage` | Coverage of reporting requirements | | `provenance_completeness` | Completeness of data provenance chain | | `chunk_diversity` | Diversity of text chunks in RAG pipelines | | `subtitle_diversity` | Diversity of section headings in reports | *** ## Fairness Metric Hierarchy [Section titled “Fairness Metric Hierarchy”](#fairness-metric-hierarchy) Different metrics capture different fairness concepts, ordered by strictness: ```plaintext Demographic Parity (least strict) | Same approval rates across groups v Equal Opportunity (medium) | Same TPR across groups v Equalized Odds (most strict) | Same TPR AND FPR across groups v Predictive Parity (orthogonal) Same precision across groups ``` In practice, a system can pass demographic parity while failing equalized odds. Choose metrics aligned with your risk context: * **Lending / Hiring:** Equalized odds + predictive parity * **Healthcare:** Equalized odds + privacy metrics (k-anonymity >= 5) * **Comprehensive audit:** All metrics together *** ## Using Metrics in OSCAL Policies [Section titled “Using Metrics in OSCAL Policies”](#using-metrics-in-oscal-policies) Every metric can be referenced in an OSCAL policy via the `metric_key` property: ```yaml assessment-plan: metadata: title: "Custom Fairness Policy" control-implementations: - description: "Fairness Controls" implemented-requirements: - control-id: my-check props: - name: metric_key value: equalized_odds_ratio # <-- Registry key - name: threshold value: "0.20" - name: operator value: "<" - name: "input:dimension" value: gender # <-- Protected attribute ``` See [Policy Authoring Guide](/guides/policy-authoring/) for the complete OSCAL format reference. *** ## Adding Custom Metrics [Section titled “Adding Custom Metrics”](#adding-custom-metrics) To register a new metric: 1. Create the function in the appropriate module under `venturalitica/assurance/`: ```python def calc_my_metric(df, **kwargs) -> float: # Validate inputs if "dimension" not in kwargs: raise ValueError("Missing 'dimension' parameter") # Calculate and return return value ``` 2. Register it in `venturalitica/metrics/__init__.py`: ```python METRIC_REGISTRY["my_metric"] = calc_my_metric ``` 3. Use it in your OSCAL policy via `metric_key: my_metric`. *** ## References [Section titled “References”](#references) * [Fairlearn](https://fairlearn.org/) — Microsoft fairness library * [AI Fairness 360](https://aif360.readthedocs.io/) — IBM fairness metrics * [GDPR Article 5](https://gdpr-info.eu/art-5-gdpr/) — Data minimization principle * [OSCAL](https://pages.nist.gov/OSCAL/) — Open Security Controls Assessment Language

# Multiclass Fairness Metrics

> 7 multiclass fairness metrics for AI systems with 3+ output classes, including intersectional analysis.

Venturalitica includes 7 multiclass fairness metrics for evaluating AI systems with more than 2 output classes (e.g., credit risk grades A/B/C/D, multi-label classification, sentiment categories). These extend traditional binary fairness concepts to multi-class settings. ## When to Use Multiclass Metrics [Section titled “When to Use Multiclass Metrics”](#when-to-use-multiclass-metrics) Use these metrics when your model produces **3+ classes**. Binary metrics like `disparate_impact` or `demographic_parity_diff` only work with 2-class outputs. Multiclass metrics aggregate fairness across all class labels. Common scenarios: * Credit risk grading (A, B, C, D, E) * Job recommendation categories * Medical diagnosis classification * Content moderation labels *** ## Metric Reference [Section titled “Metric Reference”](#metric-reference) ### 1. `multiclass_demographic_parity` [Section titled “1. multiclass\_demographic\_parity”](#1-multiclass_demographic_parity) **What it measures**: Maximum disparity in prediction rates across protected groups, aggregated over all classes using one-vs-rest decomposition. **Formula**: For each class `c`, compute `P(Y_hat=c | A=a)` for each group `a`. The disparity for class `c` is `max(rates) - min(rates)`. Return the maximum disparity across all classes. **Ideal value**: 0.0 (all groups receive each class at equal rates). **Registry key**: `multiclass_demographic_parity` **Required inputs**: `target`, `prediction`, `dimension` ```yaml - control-id: mc-demographic-parity description: "Multi-class demographic parity < 0.15" props: - name: metric_key value: multiclass_demographic_parity - name: threshold value: "0.15" - name: operator value: lt - name: "input:target" value: target - name: "input:prediction" value: prediction - name: "input:dimension" value: gender ``` *** ### 2. `multiclass_equal_opportunity` [Section titled “2. multiclass\_equal\_opportunity”](#2-multiclass_equal_opportunity) **What it measures**: Maximum disparity in true positive rates (TPR) across protected groups, using one-vs-rest decomposition. Ensures each group has equal chance of being correctly classified for each class. **Formula**: For each class `c`, compute TPR per group: `P(Y_hat=c | Y=c, A=a)`. Disparity = `max(TPRs) - min(TPRs)`. Return maximum disparity across classes. **Ideal value**: 0.0 (equal recall for all groups in every class). **Registry key**: `multiclass_equal_opportunity` **Required inputs**: `target`, `prediction`, `dimension` *** ### 3. `multiclass_confusion_metrics` [Section titled “3. multiclass\_confusion\_metrics”](#3-multiclass_confusion_metrics) **What it measures**: Per-class precision/recall and per-group accuracy. Returns a dictionary (not a scalar), useful for detailed diagnostics rather than policy thresholds. **Return type**: `Dict` with keys `per_class_metrics` (precision/recall per class) and `per_group_performance` (accuracy per group). **Registry key**: `multiclass_confusion_metrics` **Required inputs**: `target`, `prediction`, `dimension` *** ### 4. `weighted_demographic_parity_multiclass` [Section titled “4. weighted\_demographic\_parity\_multiclass”](#4-weighted_demographic_parity_multiclass) **What it measures**: Demographic parity with configurable aggregation strategy across classes. **Strategies** (set via `strategy` parameter): | Strategy | Description | | :---------------- | :---------------------------------------------------------- | | `macro` (default) | Maximum disparity across all classes | | `micro` | Maximum disparity using normalized prediction distributions | | `one-vs-rest` | Same as macro but explicit one-vs-rest decomposition | | `weighted` | Disparities weighted by class prevalence | **Formula (macro)**: Same as `multiclass_demographic_parity`, but with strategy control. **Ideal value**: 0.0 **Registry key**: `weighted_demographic_parity_multiclass` **Required inputs**: `target` (unused but validated), `prediction`, `dimension` **Minimum samples**: 30 *** ### 5. `macro_equal_opportunity_multiclass` [Section titled “5. macro\_equal\_opportunity\_multiclass”](#5-macro_equal_opportunity_multiclass) **What it measures**: Macro-averaged equal opportunity. Computes TPR disparity for each class (one-vs-rest), then returns the maximum. **Formula**: For each class `c`, binarize as `y_true_c = (y == c)`. Compute TPR per group. Disparity = `max(TPRs) - min(TPRs)`. Return `max(disparities)`. **Ideal value**: 0.0 **Registry key**: `macro_equal_opportunity_multiclass` **Required inputs**: `target`, `prediction`, `dimension` **Minimum samples**: 30 *** ### 6. `micro_equalized_odds_multiclass` [Section titled “6. micro\_equalized\_odds\_multiclass”](#6-micro_equalized_odds_multiclass) **What it measures**: Combined TPR + FPR disparity across groups. Measures whether the model’s overall accuracy and error rate are equitable across protected groups. **Formula**: For each group, compute overall accuracy and error rate. Return `(max_accuracy - min_accuracy) + (max_error_rate - min_error_rate)`. **Ideal value**: 0.0 (no accuracy/error disparity between groups). **Registry key**: `micro_equalized_odds_multiclass` **Required inputs**: `target`, `prediction`, `dimension` **Minimum samples**: 30 *** ### 7. `predictive_parity_multiclass` [Section titled “7. predictive\_parity\_multiclass”](#7-predictive_parity_multiclass) **What it measures**: Precision disparity across protected groups for each class. Ensures that when the model predicts a class, it is equally accurate for all groups. **Strategies**: `macro` (default), `weighted` **Formula (macro)**: For each class `c`, compute precision per group: `P(Y=c | Y_hat=c, A=a)`. Disparity = `max(precisions) - min(precisions)`. Return max across classes. **Ideal value**: 0.0 **Registry key**: `predictive_parity_multiclass` **Required inputs**: `target`, `prediction`, `dimension` *** ## Summary Table [Section titled “Summary Table”](#summary-table) | Registry Key | What It Checks | Ideal | Strategies | | :--------------------------------------- | :--------------------------- | :---- | :------------------------------------------ | | `multiclass_demographic_parity` | Prediction rate parity (OVR) | 0.0 | `max`, `macro` aggregation | | `multiclass_equal_opportunity` | TPR parity (OVR) | 0.0 | — | | `multiclass_confusion_metrics` | Per-class/group diagnostics | Dict | — | | `weighted_demographic_parity_multiclass` | Prediction rate parity | 0.0 | `macro`, `micro`, `one-vs-rest`, `weighted` | | `macro_equal_opportunity_multiclass` | TPR parity (macro) | 0.0 | — | | `micro_equalized_odds_multiclass` | Accuracy + error parity | 0.0 | — | | `predictive_parity_multiclass` | Precision parity | 0.0 | `macro`, `weighted` | *** ## Comprehensive Report [Section titled “Comprehensive Report”](#comprehensive-report) For a combined view, use the `calc_multiclass_fairness_report()` function in Python: ```python from venturalitica.metrics import calc_multiclass_fairness_report report = calc_multiclass_fairness_report( y_true=df["target"], y_pred=df["prediction"], protected_attr=df["gender"] ) # Returns dict with: # - weighted_demographic_parity_macro # - macro_equal_opportunity # - micro_equalized_odds # - predictive_parity_macro ``` ### Intersectional Analysis [Section titled “Intersectional Analysis”](#intersectional-analysis) For intersectional fairness (e.g., gender x age), pass multiple attributes: ```python from venturalitica.assurance.fairness.multiclass_reporting import calc_intersectional_metrics results = calc_intersectional_metrics( y_true=df["target"], y_pred=df["prediction"], protected_attrs={ "gender": df["gender"], "age_group": df["age_group"] } ) # Returns: # - intersectional_disparity: max - min accuracy across slices # - worst_slice: e.g., "female x elderly" # - best_slice: e.g., "male x young" # - slice_details: accuracy per intersection ``` *** ## Constraints [Section titled “Constraints”](#constraints) * **Minimum samples**: Most multiclass metrics require >= 30 samples and raise `ValueError` otherwise. * **Minimum groups**: At least 2 protected groups required. * **Minimum classes**: At least 2 classes required (though for 2-class problems, prefer the simpler binary metrics). * **Optional dependency**: Some metrics use Fairlearn internally. Install with `pip install fairlearn` if needed. *** ## Related [Section titled “Related”](#related) * **[Metrics Reference](/reference/metrics/)** — All 35+ metrics including binary fairness, privacy, and performance * **[Policy Authoring](/guides/policy-authoring/)** — How to use metric keys in OSCAL controls * **[Column Binding](/guides/column-binding/)** — How `dimension`, `target`, `prediction` map to columns

# Probes Reference

> 7 evidence probes for EU AI Act compliance: integrity, hardware, carbon, BOM, artifact, handshake, and trace.

The probe system powers `vl.monitor()`. When you wrap code in a monitor context, 7 probes activate automatically to capture multimodal evidence about your execution environment, data lineage, and compliance status. ```python with vl.monitor("training_run"): # All 7 probes are active here model.fit(X, y) vl.enforce(data=df, policy="policy.oscal.yaml") ``` *** ## Probe Architecture [Section titled “Probe Architecture”](#probe-architecture) All probes extend `BaseProbe` and implement: * **`start()`** — Called when the monitor context opens * **`stop()`** — Called when the monitor context closes; returns a results dict * **`get_summary()`** — Returns a human-readable one-line summary Probes are designed to be non-invasive: if a probe fails (e.g., missing optional dependency), it silently degrades without interrupting your code. *** ## Probe Reference [Section titled “Probe Reference”](#probe-reference) ### IntegrityProbe [Section titled “IntegrityProbe”](#integrityprobe) **Purpose:** Generates a SHA-256 fingerprint of the execution environment and detects drift. **EU AI Act:** Article 15 (Accuracy, Robustness & Cybersecurity) **What it captures:** | Field | Description | | :---------------- | :------------------------------------------------------------- | | `fingerprint` | SHA-256 hash of `{OS}-{Python version}-{CWD}` (first 12 chars) | | `metadata.os` | Operating system and release | | `metadata.python` | Python version | | `metadata.arch` | CPU architecture | | `metadata.node` | Machine hostname | | `metadata.cwd` | Current working directory | | `drift_detected` | `true` if fingerprint changed between start and stop | **Output example:** ```plaintext [Security] Fingerprint: 89fbf3a21c04 | Integrity: Stable ``` **Why it matters:** Proves the environment did not change during execution. If someone swaps the dataset or changes the working directory mid-run, drift is detected. *** ### HardwareProbe [Section titled “HardwareProbe”](#hardwareprobe) **Purpose:** Tracks peak RAM usage and CPU count. **EU AI Act:** Article 15 (Accuracy, Robustness & Cybersecurity) **What it captures:** | Field | Description | | :--------------- | :---------------------------- | | `peak_memory_mb` | Peak RSS memory in megabytes | | `cpu_count` | Number of available CPU cores | **Optional dependency:** `psutil` (degrades gracefully if not installed) **Output example:** ```plaintext [Hardware] Peak Memory: 256.42 MB | CPUs: 8 ``` *** ### CarbonProbe [Section titled “CarbonProbe”](#carbonprobe) **Purpose:** Tracks CO2 emissions during training using CodeCarbon. **EU AI Act:** Article 15 (Robustness — environmental impact reporting) **What it captures:** | Field | Description | | :------------- | :--------------------------------------------- | | `emissions_kg` | Estimated carbon emissions in kilograms of CO2 | **Optional dependency:** `codecarbon` (shows warning if not installed) **Output example:** ```plaintext [Green AI] Carbon emissions: 0.000042 kgCO2 ``` **Why it matters:** Some regulatory frameworks and ESG reporting require carbon impact disclosure for compute-intensive AI training. *** ### BOMProbe [Section titled “BOMProbe”](#bomprobe) **Purpose:** Captures a Software Bill of Materials (SBOM) at runtime. **EU AI Act:** Article 13 (Transparency & Information) **What it captures:** | Field | Description | | :---------------- | :------------------------------------------- | | `component_count` | Number of Python packages in the environment | | `bom` | Full SBOM as JSON (CycloneDX-compatible) | | `bom_path` | File path where the SBOM was saved | **Where it saves:** `{session_dir}/bom.json` or `.venturalitica/bom.json` **Output example:** ```plaintext [Supply Chain] BOM Captured: 142 components linked. ``` **Why it matters:** Maps to Article 13 transparency requirements. The SBOM proves exactly which library versions were used, enabling supply chain vulnerability auditing (CVE scanning). *** ### ArtifactProbe [Section titled “ArtifactProbe”](#artifactprobe) **Purpose:** Tracks input and output artifacts for data lineage. **EU AI Act:** Article 10 (Data & Data Governance) **Constructor parameters:** | Parameter | Type | Description | | :-------- | :-------------------- | :--------------------------------------- | | `inputs` | `List[str]` or `None` | Paths to input files (datasets, configs) | | `outputs` | `List[str]` or `None` | Paths to output files (models, plots) | **What it captures:** | Field | Description | | :-------- | :---------------------------------------------------------- | | `inputs` | Snapshot of input artifacts at start (name, hash, metadata) | | `outputs` | Snapshot of output artifacts at stop | **Usage:** ```python with vl.monitor("training", inputs=["data/train.csv"], outputs=["models/credit_model.pkl"]): model.fit(X, y) ``` **Output example:** ```plaintext [Artifacts] Inputs: 1 | Outputs: 1 (Deep Integration) ``` *** ### HandshakeProbe [Section titled “HandshakeProbe”](#handshakeprobe) **Purpose:** Checks whether `vl.enforce()` was called inside the monitor session. **EU AI Act:** Article 9 (Risk Management System) **What it captures:** | Field | Description | | :--------------- | :---------------------------------------------------------------- | | `is_compliant` | `true` if `enforce()` was called at any point | | `newly_enforced` | `true` if `enforce()` was called during this session (not before) | **Output example (no enforcement detected):** ```plaintext [Handshake] Nudge: No policy enforcement detected yet. Run `vl.enforce()` to ensure compliance. ``` **Output example (enforcement detected):** ```plaintext [Handshake] Policy enforced verifyable audit trail present. ``` **Why it matters:** Promotes the compliance workflow. If a developer uses `monitor()` for training but forgets to call `enforce()`, the handshake probe nudges them toward policy enforcement. *** ### TraceProbe [Section titled “TraceProbe”](#traceprobe) **Purpose:** Captures logical execution evidence including AST code analysis, timestamps, and call context. **EU AI Act:** Articles 10 & 11 (Data Governance & Technical Documentation) **Constructor parameters:** | Parameter | Type | Description | | :--------- | :-------------- | :------------------------------------- | | `run_name` | `str` | Name for this trace (used in filename) | | `label` | `str` or `None` | Optional categorization label | **What it captures:** | Field | Description | | :---------------------- | :-------------------------------------------------------------- | | `name` | The run name | | `label` | Optional label | | `timestamp` | ISO-8601 timestamp when the session ended | | `duration_seconds` | Wall-clock execution time | | `success` | `true` if no exception was raised | | `code_context.file` | Name of the user script that called `monitor()` | | `code_context.analysis` | AST analysis of the script (function calls, imports, structure) | **Where it saves:** `{session_dir}/trace_{run_name}.json` or `.venturalitica/trace_{run_name}.json` **Output example:** ```plaintext [Trace] Context: train_model.py | Evidence saved to .venturalitica/trace_credit_model_v1.json ``` **Why it matters:** The trace file is the core evidence artifact. It proves not just the results, but HOW they were computed — which script was run, how long it took, and whether it succeeded. *** ## Evidence Directory Structure [Section titled “Evidence Directory Structure”](#evidence-directory-structure) After a `monitor()` session, evidence is saved to: ```plaintext .venturalitica/ results.json # enforce() results (cumulative) trace_{run_name}.json # TraceProbe output bom.json # BOMProbe output sessions/ {session_id}/ results.json # Session-specific enforce() results trace_{run_name}.json # Session-specific trace bom.json # Session-specific SBOM ``` The Dashboard reads these files to populate Phase 3 (Verify & Evaluate). *** ## Probe Dependencies [Section titled “Probe Dependencies”](#probe-dependencies) | Probe | Required | Optional Dependency | | :------------- | :------- | :------------------------------------ | | IntegrityProbe | Built-in | — | | HardwareProbe | Built-in | `psutil` (for memory/CPU data) | | CarbonProbe | Built-in | `codecarbon` (for emissions tracking) | | BOMProbe | Built-in | — | | ArtifactProbe | Built-in | — | | HandshakeProbe | Built-in | — | | TraceProbe | Built-in | — | Install optional dependencies: ```bash pip install psutil codecarbon ```

# Writing Code-First Policy (The Engineer)

> Learn to translate legal requirements into technical rules using OSCAL policy files.

This guide focuses on the **Engineer Persona**: the one who translates legal requirements into technical rules (OSCAL). In **Level 1**, you learned to “Block” bad deployments. Now we will write the actual policy file that governs the project. *** ## The Policy File (`data_policy.yaml`) [Section titled “The Policy File (data\_policy.yaml)”](#the-policy-file-data_policyyaml) For **Phase 1 (Data Audit)**, we only care about **Article 10 (Data Assurance)**. Your Data Scientist (The Builder) cannot start training until this file is ready. ### 1. The Structure [Section titled “1. The Structure”](#1-the-structure) Create a file named `data_policy.yaml` in your project root. ```yaml assessment-plan: metadata: title: "Article 10: Consumer Credit Directive (CCD)" description: "Acceptance criteria for training data quality and bias." control-implementations: - description: "Data Quality & Fairness Controls" implemented-requirements: # RULES GO HERE ``` *** ## 2. Defining the Rules (Controls) [Section titled “2. Defining the Rules (Controls)”](#2-defining-the-rules-controls) A “Control” is a unit of logic. In the EU AI Act, you must prove you checked for specific risks. ### Rule A: Representation (Statistical Support) [Section titled “Rule A: Representation (Statistical Support)”](#rule-a-representation-statistical-support) * **Legal Requirement**: “Training, validation and testing data sets shall be relevant, representative, free of errors and complete.” (Art 10.3) * **Translation**: Ensure no demographic group is erased (Min 20% representation). ```yaml - control-id: check-imbalance description: "Ensure minority groups are statistically significant." props: - name: metric_key value: class_imbalance - name: threshold value: "0.20" # Fail if minority class < 20% - name: operator value: gt ``` ### Rule B: Bias (Disparate Impact) [Section titled “Rule B: Bias (Disparate Impact)”](#rule-b-bias-disparate-impact) * **Legal Requirement**: “Examination of possible biases.” (Art 10.2.f) * **Translation**: Acceptance rates must not deviate by more than 20% between groups (Four-Fifths Rule). ```yaml - control-id: check-gender-bias description: "Disparate Impact Ratio must be within 0.8 - 1.25" props: - name: metric_key value: disparate_impact - name: threshold value: "0.80" - name: operator value: gt - name: "input:dimension" value: gender ``` *** ## 3. Verify the Policy [Section titled “3. Verify the Policy”](#3-verify-the-policy) Before handing it off to the Data Scientist, verify it works. ```python import venturalitica as vl from venturalitica.quickstart import load_sample # 1. Load the 'Approved' Dataset (Mock) data = load_sample('loan') # 2. Dry Run the Policy try: vl.enforce( data=data, target="class", gender="Attribute9", # "Personal status and sex" in German Credit Data policy="data_policy.yaml" ) print("Policy is valid syntax and passes baseline data.") except Exception as e: print(f"Policy Error: {e}") ``` *** ## Part 2: The Model Policy (`model_policy.yaml`) [Section titled “Part 2: The Model Policy (model\_policy.yaml)”](#part-2-the-model-policy-model_policyyaml) Once the data is approved, you need to define the rules for the **final product** (the trained model). This corresponds to **Article 15 (Accuracy, Robustness, and Cybersecurity)**. Create a second file: `model_policy.yaml`. ### Rule C: Performance (Accuracy) [Section titled “Rule C: Performance (Accuracy)”](#rule-c-performance-accuracy) * **Legal Requirement**: “High-risk AI systems shall be designed … to achieve an appropriate level of accuracy.” (Art 15.1) * **Translation**: The model must be better than random guessing (e.g., > 70% accuracy). ```yaml - control-id: accuracy-check description: "Model must achieve at least 70% accuracy." props: - name: metric_key value: accuracy_score - name: threshold value: "0.70" - name: operator value: gt - name: "input:target" value: target - name: "input:prediction" value: prediction ``` ### Rule D: Post-Training Fairness (Outcome) [Section titled “Rule D: Post-Training Fairness (Outcome)”](#rule-d-post-training-fairness-outcome) * **Legal Requirement**: “Results shall not be biased…” * **Translation**: Even if data was balanced, the model might still learn to discriminate. Check the predictions again. ```yaml - control-id: gender-fairness-model description: "Ensure model predictions do not disparately impact women." props: - name: metric_key value: disparate_impact - name: "input:dimension" value: gender - name: threshold value: "0.80" - name: operator value: gt ``` *** ## What’s Next? [Section titled “What’s Next?”](#whats-next) You have now created the **specification**. Hand these files (`data_policy.yaml` and `model_policy.yaml`) to your Data Scientist. They will use them in **[Level 2](/academy/level2-integrator/)** to audit their training pipeline.